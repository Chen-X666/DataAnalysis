{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/CBravoR/AdvancedAnalyticsLabs/blob/master/notebooks/python/Lab_9_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwgnoEJt2iE_"
   },
   "source": [
    "# Embeddings\n",
    "\n",
    "In this lab we will use an embedding to train a simple model over the IMDB dataset, but we will use an embedding instead of a one-hot representation. For this, we will use the fastText embeddings that have been provided.\n",
    "\n",
    "First, let's start by importing the data and the packages that we will use. Remember to set the runtime environment to GPU!\n",
    "\n",
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_layVxFU2_6a"
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.feature_extraction as skprep\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from itertools import compress\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "random.seed(2022)\n",
    "\n",
    "# Import relevant packages\n",
    "import os\n",
    "import codecs\n",
    "%matplotlib inline\n",
    "\n",
    "# Keras imports\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Reshape, MaxPooling1D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, Lambda\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy, categorical_accuracy\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing import image, sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "__0Lf_Hv3EAk",
    "outputId": "3ce7f60f-b729-4de6-99d0-d34a79b47270"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-08 16:49:28--  https://docs.google.com/spreadsheets/d/e/2PACX-1vRoGKr2UZzLvSBPyjnx96EU4109DW34ljLWKk7abK3y4qrPLLUmEXPx8CtGEsaZswQwRJ5xMH1dguS4/pub?gid=941850445&single=true&output=csv\n",
      "Resolving docs.google.com (docs.google.com)... 172.217.212.139, 172.217.212.102, 172.217.212.101, ...\n",
      "Connecting to docs.google.com (docs.google.com)|172.217.212.139|:443... connected.\n",
      "HTTP request sent, awaiting response... 307 Temporary Redirect\n",
      "Location: https://doc-10-9o-sheets.googleusercontent.com/pub/70cmver1f290kjsnpar5ku2h9g/cjp0qtjb0ai7hl1q1vp89j4ltg/1646758165000/108328119934179437001/*/e@2PACX-1vRoGKr2UZzLvSBPyjnx96EU4109DW34ljLWKk7abK3y4qrPLLUmEXPx8CtGEsaZswQwRJ5xMH1dguS4?gid=941850445&single=true&output=csv [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2022-03-08 16:49:28--  https://doc-10-9o-sheets.googleusercontent.com/pub/70cmver1f290kjsnpar5ku2h9g/cjp0qtjb0ai7hl1q1vp89j4ltg/1646758165000/108328119934179437001/*/e@2PACX-1vRoGKr2UZzLvSBPyjnx96EU4109DW34ljLWKk7abK3y4qrPLLUmEXPx8CtGEsaZswQwRJ5xMH1dguS4?gid=941850445&single=true&output=csv\n",
      "Resolving doc-10-9o-sheets.googleusercontent.com (doc-10-9o-sheets.googleusercontent.com)... 209.85.145.132, 2607:f8b0:4001:c1e::84\n",
      "Connecting to doc-10-9o-sheets.googleusercontent.com (doc-10-9o-sheets.googleusercontent.com)|209.85.145.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/csv]\n",
      "Saving to: ‘texts.csv’\n",
      "\n",
      "texts.csv               [  <=>               ]   1.18M  4.76MB/s    in 0.2s    \n",
      "\n",
      "2022-03-08 16:49:29 (4.76 MB/s) - ‘texts.csv’ saved [1235939]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate --output-document=texts.csv 'https://docs.google.com/spreadsheets/d/e/2PACX-1vRoGKr2UZzLvSBPyjnx96EU4109DW34ljLWKk7abK3y4qrPLLUmEXPx8CtGEsaZswQwRJ5xMH1dguS4/pub?gid=941850445&single=true&output=csv'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "texts = pd.read_csv('texts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNRQEl1f6PJz"
   },
   "source": [
    "With this we are ready to start training embeddings!\n",
    "\n",
    "# fasttext\n",
    "\n",
    "We will use [fasttext embeddings](https://github.com/facebookresearch/fastText) in this lab. For this, we need to download the fasttext model and apply it to our data, i.e., associate each word with the corresponding embedding vector.\n",
    "\n",
    "fasttext is a heavy program, so it makes sense to use the C++ library directly. This does complicate our life a bit, but nothing a few lines of code can't solve. First, we will download the library and unzip it as before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SHzjAjgB6BMS"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/facebookresearch/fastText/archive/v0.9.1.zip\n",
    "  \n",
    "!unzip v0.9.1.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAENl0CT7_UC"
   },
   "source": [
    "Now we need to [compile](https://en.wikipedia.org/wiki/Compiler) the library. Compiling turns the code we just downloaded to something the computer can understand. Configuring this is complicated, so programmers add a [makefile](http://www.cs.colby.edu/maxwell/courses/tutorials/maketutor/) with instructions for the compiler. This means we just need to call the command ```make``` in the base folder with the code. This code does it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_WfqT14678gq"
   },
   "outputs": [],
   "source": [
    "%cd fastText-0.9.1\n",
    "\n",
    "!make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2I4kf32v_kFc"
   },
   "source": [
    "There are some warnings, but we can ignore them as they are for future versions.\n",
    "\n",
    "Now, we need to download the embedding vectors. These are **really heavy downloads** of about 8GB.  fasttext is a language-dependent model, so be sure to download the one for your chosen application. The list is [here](https://fasttext.cc/docs/en/english-vectors.html). There are four levels of embeddings:\n",
    "\n",
    "- Vectors trained over Wikipedia (1 million words, 16 billion tokens).\n",
    "- Vectors trained over Wikipedia with subword information.\n",
    "- Vectors trained over a webcrawl of many websites ([Common Crawl](http://commoncrawl.org/)). This one has 2M words and 200B tokens.\n",
    "- Vectors trained over a webcrawl of many websites with subword information. \n",
    "\n",
    "For the lab we will use the first one, but I encourage you to use try other ones! The download will take a little while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3AYbQEvv_aJy",
    "outputId": "11cf90d9-981e-4fb1-81fc-5f81356fa241"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-08 16:50:04--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 172.67.9.4, 104.22.75.142, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4503593528 (4.2G) [application/octet-stream]\n",
      "Saving to: ‘cc.en.300.bin.gz’\n",
      "\n",
      "cc.en.300.bin.gz    100%[===================>]   4.19G  54.5MB/s    in 90s     \n",
      "\n",
      "2022-03-08 16:51:35 (47.7 MB/s) - ‘cc.en.300.bin.gz’ saved [4503593528/4503593528]\n",
      "\n",
      "cc.en.300.bin.gz:\t 37.8% -- replaced with cc.en.300.bin\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
    "  \n",
    "!gunzip -v -f cc.en.300.bin.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtAb3nGqEG1X"
   },
   "source": [
    "Now we are ready to go!\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "With this ready we can now start working on the data, which comes from the [Internet Movie Database](https://www.imdb.com/) (IMDB). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pgDDmGJhFxl8",
    "outputId": "17e3087c-3d36-413b-b484-5ce462193bf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "# Come back to the original work folder\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "s5OWCnSmhi3r",
    "outputId": "eee24a25-168c-4fce-a4ee-cbbf85d24a66"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-26572840-f1aa-46ed-a469-fc20f9e1239f\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the best ensemble acted films I've ever...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As it was already put, the best version ever o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I think this movie is absolutely beautiful. An...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This for me was a wonderful introduction to th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This movie of 370 minutes was aired by the Ita...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Ah, Bait. How do I hate thee? Let me count the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>They constructed this one as a kind of fantasy...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>It was 9:30 PM last night at my friend's campi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The worst thing about Crush is not that it's a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Shortly after seeing this film I questioned th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-26572840-f1aa-46ed-a469-fc20f9e1239f')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-26572840-f1aa-46ed-a469-fc20f9e1239f button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-26572840-f1aa-46ed-a469-fc20f9e1239f');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                                 texts  class\n",
       "0    one of the best ensemble acted films I've ever...      1\n",
       "1    As it was already put, the best version ever o...      1\n",
       "2    I think this movie is absolutely beautiful. An...      1\n",
       "3    This for me was a wonderful introduction to th...      1\n",
       "4    This movie of 370 minutes was aired by the Ita...      1\n",
       "..                                                 ...    ...\n",
       "995  Ah, Bait. How do I hate thee? Let me count the...      0\n",
       "996  They constructed this one as a kind of fantasy...      0\n",
       "997  It was 9:30 PM last night at my friend's campi...      0\n",
       "998  The worst thing about Crush is not that it's a...      0\n",
       "999  Shortly after seeing this film I questioned th...      0\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1Tx9uXbGOkQ"
   },
   "source": [
    "Now we'll clean the text, getting rid of special characters and keeping the text in a clear form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "6Q8yPXEIFn-b",
    "outputId": "7439486e-8ee6-4f7c-c200-9caed12ed180"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-4775e092-6490-4ef9-9d0a-c8ad05af4c17\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the best ensemble acted films Ive ever ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As it was already put the best version ever of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I think this movie is absolutely beautiful And...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This for me was a wonderful introduction to th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This movie of 370 minutes was aired by the Ita...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4775e092-6490-4ef9-9d0a-c8ad05af4c17')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-4775e092-6490-4ef9-9d0a-c8ad05af4c17 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-4775e092-6490-4ef9-9d0a-c8ad05af4c17');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                               texts  class\n",
       "0  one of the best ensemble acted films Ive ever ...      1\n",
       "1  As it was already put the best version ever of...      1\n",
       "2  I think this movie is absolutely beautiful And...      1\n",
       "3  This for me was a wonderful introduction to th...      1\n",
       "4  This movie of 370 minutes was aired by the Ita...      1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text cleaning\n",
    "import string\n",
    "\n",
    "# Collect punctuation signs.\n",
    "table = str.maketrans(' ', ' ', string.punctuation)\n",
    "\n",
    "# Remove them from the text\n",
    "texts.iloc[:,0] = [j.translate(table) for j in texts.iloc[:,0]]\n",
    "texts.iloc[:,0] = [j.replace('\\x96',' ') for j in texts.iloc[:,0]]\n",
    "\n",
    "# Eliminate double spaces\n",
    "texts.iloc[:,0] = [\" \".join(j.split()) for j in texts.iloc[:,0]]\n",
    "\n",
    "# Show first 5\n",
    "texts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bN6R_l89HKeJ"
   },
   "source": [
    "## Estimating the embedding\n",
    "\n",
    "Once we have the embedding model, using it consists of:\n",
    "\n",
    "1. Calculate the words that appear on the text and save to disk\n",
    "2. Use the fastText program to obtain the word embeddings.\n",
    "3. Import the embeddings into a Keras input layer.\n",
    "4. Train the model!\n",
    "\n",
    "First, we will start by selecting the individual words. The Keras internal model \"[Tokenizer](https://keras.io/preprocessing/text/#tokenizer)\" will allow us to quickly do this, with the added benefit of giving us a [dictionary](https://docs.python.org/2/tutorial/datastructures.html#dictionaries) of the words, which will be stored in the \"tokenizer\" model.\n",
    "\n",
    "A dictionary is a very powerful object included by Python, which will efficiently index anything by any key. In our case it will index the words and an arbitraty number that will give its position. Read the linked article to know more about dictionaries, but it is important that you understand its usefulness: It allows fast (indexed) access to objects linked by a key (in this case, the words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "O6owu-t2G0Z7"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer() # Creates tokenizer model.\n",
    "tokenizer.fit_on_texts(texts.iloc[:,0]) # Trains it over the tokens that we have.\n",
    "\n",
    "# Get words\n",
    "Vals = list(tokenizer.word_index.keys())\n",
    "\n",
    "# Write CSV with the output.\n",
    "file = codecs.open('IMDBWords.csv', \"w\", \"utf-8\")\n",
    "\n",
    "for item in Vals:\n",
    "    file.write(\"%s\\r\\n\" % item)\n",
    "    \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yBLb3CX9Hiyj",
    "outputId": "da5dd0e2-7441-4098-ba6b-3fb86e7895e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastText-0.9.1\tIMDBWords.csv  sample_data  texts.csv  v0.9.1.zip\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hqm-JobbHzXH",
    "outputId": "ec760412-bacc-48ce-fc26-0cc5e9aa588f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\r\n",
      "and\r\n",
      "a\r\n",
      "of\r\n",
      "to\r\n",
      "is\r\n",
      "in\r\n",
      "it\r\n",
      "i\r\n",
      "this\r\n"
     ]
    }
   ],
   "source": [
    "!head IMDBWords.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eR7L1c4pHse0"
   },
   "source": [
    "We now have a csv file with all the words being used to review the movies in a standard format. Let's get the embeddings!\n",
    "\n",
    "We need to call the fasttext software from the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mwRTWVwWH6GK",
    "outputId": "17521fce-752d-4612-d85f-8057ab856ca1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 4800004096 bytes == 0x559b6a208000 @  0x7f173e0c2887 0x559b574578cf 0x559b574623b4 0x559b57462ea1 0x559b5746e843 0x559b57439ef8 0x7f173d15fbf7 0x559b57439f8a\n",
      "tcmalloc: large alloc 2400002048 bytes == 0x559c883ac000 @  0x7f173e0c2887 0x559b574578cf 0x559b574623f8 0x559b57462ea1 0x559b5746e843 0x559b57439ef8 0x7f173d15fbf7 0x559b57439f8a\n"
     ]
    }
   ],
   "source": [
    "!./fastText-0.9.1/fasttext print-sentence-vectors fastText-0.9.1/cc.en.300.bin < IMDBWords.csv > EmbeddingIMDB.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1UwGcBfEIlbe"
   },
   "source": [
    "As always, ignore the warnings. They are basically saying \"I need a very large amount of RAM to do what you are asking!\".\n",
    "\n",
    "This process actually takes a relatively long time. Let's take a look at the command part by part:\n",
    "\n",
    "- ```!./fastText-0.2.0/fasttext``` invokes fasttext. The notation \"./\" means \"execute this program\".\n",
    "\n",
    "- We give it two parameters ```print-sentence-vectors``` which instruct fastText to actually give us the embedding for every word, and ```fastText-0.2.0/cc.en.300.bin``` which is the language model we are using.\n",
    "\n",
    "- Then comes the processing of the inputs and outputs. The \"```< IMDBWords.csv```\" is telling Linux \"give IMDBWords.csv as an input to what's to the left\" and the \"```> EmbeddingIMDB.tsv```\" is telling Linux \"write whatever is outputted from the left into EmbeddingIMDB.tsv\".\n",
    "\n",
    "The output is a space-separated file with the embedding vectors in the same order we gave them to the software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OV434-joIQSK",
    "outputId": "ca062812-3032-4db0-a94c-cf88a4c24f38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.040902 0.058467 -0.010321 0.035355 -0.027129 0.016771 0.0054634 -0.012907 -0.014288 -0.0015782 -0.080665 0.0046677 0.020279 -0.0020526 -0.046287 -0.029847 0.012894 0.011567 -0.006924 -0.013908 -0.0067564 -0.0061605 -0.014448 0.0069548 0.0010292 -0.07417 0.010987 0.011773 -0.031122 -0.023272 0.0074486 -0.019942 -0.0082534 -0.17495 -0.01807 -0.0070631 -0.025471 0.064953 0.00166 0.02227 0.0056682 -0.0072136 -0.027801 -0.014074 -0.055771 0.049818 -0.007309 -0.017649 -0.0044152 0.040688 -0.024261 0.034503 -0.0086712 -0.043835 0.0070655 -0.053175 0.008306 0.045381 0.0078419 -0.022345 0.037185 0.0041834 0.0024069 0.00056557 0.035013 0.0054505 -0.026406 0.0071596 -0.0059939 0.0052182 0.072519 0.024593 0.042916 0.02227 -0.015782 -0.026384 0.0041797 0.028758 0.1778 0.073325 -0.0096955 0.0067667 -0.047348 0.053457 0.031812 0.00092231 0.036672 -0.034541 0.0046972 0.072503 -0.032606 -0.011956 -0.018245 0.0075088 0.046505 0.022023 0.051143 -0.044863 -0.010241 0.037496 0.02796 -0.0095708 -0.0060604 -0.10324 0.010627 -0.03997 0.0087579 0.0093861 -0.017477 0.031134 0.017434 0.019394 0.003093 0.090583 0.018057 -0.036964 -0.036321 -0.014978 0.0060368 -0.023885 -0.027523 -0.022826 -0.031551 0.019329 -0.008056 0.045678 -0.030662 -0.0092351 -0.024067 0.19493 -0.0087554 0.028139 0.0036521 0.16553 -0.080795 0.026553 0.054415 -0.055961 0.021239 -0.033446 0.0060756 -0.021134 0.0057236 0.0027277 0.027719 -0.0050092 -0.35273 0.0082917 -0.0094937 -0.035486 -0.13408 0.039838 0.073489 -0.0034581 -0.0032121 0.025436 0.16019 0.04842 -0.023324 0.017987 -0.015021 0.013696 0.11703 -0.013808 -0.0099025 0.05427 0.026326 -0.023916 0.033809 0.0040515 0.018049 0.008191 0.057818 0.0062062 -0.0040138 0.042944 -0.025664 0.040433 0.022779 -0.046318 -4.8883e-06 0.038988 0.013122 -0.011292 0.028368 0.042914 -0.00036241 -0.046587 0.012793 -0.017523 -0.015724 0.018558 -0.053632 0.014115 0.0025984 0.0089949 0.037397 -0.035029 0.025535 0.015407 -0.051156 0.26783 0.05523 -0.017034 -0.019293 -0.0026699 -0.0027113 -0.049131 0.0096978 0.029661 -0.015567 0.019077 -0.069306 0.015859 -0.0048586 -0.020223 -0.015135 -0.020899 0.015032 -0.033394 0.019819 0.065204 -0.0076069 0.10178 0.049086 0.04254 0.014913 0.033354 0.14251 -0.00081 -0.025941 -0.0442 -0.012405 0.038703 0.02784 -0.033001 0.012574 -0.060568 -0.051953 0.039296 0.0080913 0.1163 -0.056167 -0.1161 0.37443 -0.013377 -0.0040209 0.012543 0.043473 -0.050154 -0.016603 0.0096603 0.02126 0.0047077 0.052486 0.0083836 -0.055752 -0.016397 -0.061999 -0.023006 -0.022399 -0.12395 -0.0311 0.0039879 0.016163 -0.002023 0.034499 0.022067 -0.031032 0.02902 -0.0032984 -0.01236 -0.057963 -0.12938 0.051565 -0.0049257 -0.051383 -0.15686 -0.032455 -0.12125 0.0015869 0.010518 -0.18687 -0.04173 -0.0033257 -0.035298 0.0088569 -0.026278 -0.043488 0.0010525 0.013374 -0.034663 -0.045676 0.017632 -0.061425 -0.034148 -0.019806 0.18736 0.00035375 -0.0033145 \n",
      "0.0077348 -0.0844 0.024902 -0.0080303 -0.057209 0.0063456 0.061239 0.0099712 -0.044617 -0.0071456 -0.0021205 0.0008528 -0.0062793 -0.021201 -0.006211 -0.068044 0.019064 0.020137 -0.047637 0.036793 0.050057 -0.0042504 -0.0017546 0.052305 0.013753 -0.053565 0.00046371 -0.0066575 0.013947 -0.049597 0.026741 -0.046588 0.020844 0.055057 0.02002 0.0074293 0.017587 0.086267 -0.026084 0.022481 0.039969 -0.0093332 -0.020981 0.080347 -0.042025 -0.013319 -0.00011912 -0.094103 -0.011673 0.055904 0.022154 0.014672 0.018237 0.0080571 0.014226 0.039062 0.018594 -0.0070044 -0.028887 -0.019764 0.025754 -0.039162 -0.033438 -0.020375 0.012944 0.025565 0.027335 0.015244 0.020964 0.029291 -0.0070682 0.0096305 0.027635 0.011824 0.068422 -0.011683 -0.041644 0.031046 -0.091607 -0.024302 0.0050185 0.01648 -0.018306 0.028198 -0.029945 0.026387 0.051451 -0.014319 -0.041018 -0.050798 0.010539 0.020287 -0.033841 -0.013599 -0.0033988 0.018373 0.011431 -0.053602 -0.025357 0.059908 0.040233 -0.057 -0.029984 0.0029986 -0.01171 -0.06695 -0.028523 -0.011452 0.019884 -0.025299 0.0076735 -0.10848 0.00071592 0.084106 -0.015493 -0.011988 -0.038615 -0.012667 0.01447 -0.026373 0.058036 -0.0034458 -0.076167 0.019477 0.019007 0.055125 -0.015522 0.1198 -0.029747 -0.0036906 -0.041256 0.0138 -0.055065 -0.017489 0.011841 -0.0050809 -0.040001 0.056861 -0.040172 -0.027813 -0.0096248 -0.01692 0.0096287 0.028173 0.019335 0.023168 -0.34639 -0.0036149 0.02557 -0.023604 -0.037656 0.023819 -0.041261 -0.0035723 -0.0001479 0.013 0.057116 0.030595 -0.081641 -0.0044445 -0.054844 -0.022819 -0.087701 -0.0027246 -0.0026837 0.018613 0.014004 -0.038361 0.016991 -0.019694 -0.020774 0.015811 0.029177 0.031326 -0.047354 0.015371 -0.02935 0.093974 0.026554 -0.079252 0.015809 0.020847 -0.058644 -0.058825 0.028877 0.02498 -0.058841 -0.036648 0.021992 -0.047323 -0.041756 0.044525 -0.042462 0.038603 0.016339 -0.057955 0.059872 -0.061871 -0.013987 0.06434 -0.068483 -0.01658 0.0042792 -0.012916 -0.015104 -0.017309 -0.092881 0.0065289 0.050984 0.1199 0.033415 -0.0025181 0.080571 0.010451 -0.0018394 0.02565 0.047077 -0.036321 0.030346 0.039179 0.049149 0.026013 -0.013542 0.040634 0.040091 -0.010066 0.037116 -0.048651 -0.031297 -0.010997 -0.026967 -0.0227 0.034727 0.042223 0.029635 -0.049478 0.011202 0.046969 0.01332 0.030298 0.024114 -0.0018712 0.052925 -0.22494 0.5818 0.008086 0.034386 -0.039591 0.069753 0.017664 -0.040556 -0.011573 -0.020411 -0.042889 0.011886 -0.044638 -0.099456 0.010319 -0.0013186 -0.0087211 0.036302 0.08837 -0.041459 0.015912 -0.066909 0.070271 0.0026919 0.087528 -0.10913 0.0099853 -0.014125 0.052365 -0.054929 -0.017938 0.021441 0.028557 0.017762 -0.029927 -0.052941 0.054509 0.0056185 0.067787 -0.14427 0.02669 0.010511 0.039197 0.033244 -0.090301 -0.00079008 0.018471 0.057005 -0.033333 -0.022362 -0.026056 -0.02443 -0.035722 -0.019811 -0.010882 -0.038611 0.034033 \n",
      "0.026715 -0.15116 -0.015197 -0.028548 -0.014381 -0.0064371 0.079975 0.0081964 -0.027421 -0.011053 -0.0093345 -0.0043393 0.010266 0.030503 -0.04643 0.19088 -0.0035011 -0.0019071 0.00032325 0.0038223 -0.018036 0.024013 -0.0045595 -0.029075 0.00083084 0.0023315 -0.0037382 -0.0011429 0.00065664 0.062964 0.017787 0.0056634 0.018533 -0.067961 0.011556 -0.013282 0.0089305 -0.11376 0.052138 -0.00094722 -0.035845 0.027098 -0.0032741 0.0088043 0.034597 0.096793 -0.010986 0.035893 -0.00096634 -0.0087211 0.025194 0.008966 -0.039655 -0.018805 -0.0057315 0.030778 -0.011655 -0.073524 -0.067617 -0.019679 -0.012626 -0.016535 0.0095236 -0.13077 -0.034837 -0.039468 -0.012611 -0.01836 0.024338 -0.017558 -0.0063015 0.019942 -0.0017593 -0.027478 -0.021737 -0.029455 0.032975 0.021377 0.13925 0.008019 -0.0031809 -0.0063087 -0.00087741 0.0056364 0.14064 -0.010896 0.056333 0.018397 0.029947 -0.050573 -0.01512 -0.031967 0.017024 -0.018417 -0.083044 0.089182 -0.060707 0.0014469 -0.018677 -0.0060007 0.0090612 0.039657 0.023986 -0.0026608 -0.0072448 0.039513 0.0022398 0.014787 -0.016954 0.0094188 -0.024414 0.019494 -0.020215 -0.054551 0.0019112 -0.021841 -0.012878 0.043814 -0.028128 -0.010027 0.025528 0.025829 0.014608 -0.0048467 -0.0060207 -0.01386 0.017244 0.096654 -0.0040192 -0.00063078 -0.00090667 -0.032155 0.06623 0.1194 -0.0018305 0.0027202 -0.030933 0.026431 0.048738 -0.017289 0.017267 -0.0067609 0.010445 0.022023 -0.034591 -0.0016597 -0.24926 0.013919 -0.011791 0.012964 -0.31466 0.0040374 -0.12843 0.031157 -0.010014 -0.010276 0.052264 -0.11334 0.0047082 0.037061 0.029238 -0.016841 0.12161 0.0080836 0.042446 -0.027803 0.020447 0.017465 -0.016007 -0.021664 0.00469 -0.021434 0.027266 -0.04141 -0.014027 -0.0139 0.011159 -0.008685 0.017792 0.014605 0.027928 0.093544 -0.048764 0.017723 -0.013866 0.057337 0.082752 -0.013154 -0.01051 -0.0054995 -0.0042343 -0.010476 0.059732 -0.0085056 0.023191 0.002557 -0.027898 -0.0063789 0.013925 -0.079261 -0.012303 0.39034 0.013746 0.0041312 0.026987 0.012475 0.084788 -0.022082 0.002718 -0.069223 -0.042352 -0.010135 -0.098354 0.0035687 -0.016624 0.00033379 0.024881 0.015196 0.034739 0.078432 0.015737 0.020636 0.014384 0.021712 0.02472 -0.046655 0.0074236 -0.016624 0.005391 0.0040413 -0.0098199 0.027246 -0.010168 -0.03437 -0.0069535 0.085879 -0.0055935 -0.0036963 -0.043878 0.0074741 0.017868 0.029881 0.027658 -0.01773 0.38271 -0.039949 -0.016775 0.049084 -0.057255 0.0035054 0.00041894 0.053613 0.0074797 -0.041262 -0.0085846 0.034395 -0.027638 0.014838 -0.036409 0.04402 -0.046052 0.003076 -0.016255 -0.041869 -0.0080933 0.014526 0.034098 -0.053501 0.023397 -0.10689 -0.05918 -0.027258 0.038108 0.067833 -0.024031 -0.061892 -0.060041 0.024911 0.039414 -0.057166 0.034299 0.016388 -0.082726 -0.042174 0.013516 -0.017975 -0.011367 -0.014439 -0.02583 0.016608 0.13067 0.04276 -0.049022 0.042938 0.0033228 -0.0018779 -0.007374 0.16303 -0.017012 -0.0050595 \n",
      "-2.3796e-05 -0.061166 -0.023149 -0.15093 6.4452e-05 0.0037489 -0.019147 0.018709 -0.0089672 -0.0011583 -0.0031905 -0.013487 0.030455 0.017617 -0.063841 -0.094792 -0.020604 0.015176 -0.005218 0.01002 -0.022128 0.030197 -0.0044208 -0.0046505 -0.080304 -0.039789 -0.009955 -0.0063664 -0.0077789 0.063158 0.027923 -0.0075391 0.037325 -0.054372 -0.018374 -0.022694 0.13541 0.031595 -0.012439 -0.017586 -0.010651 0.033728 0.048082 -0.0093211 -0.031841 -0.19096 -0.011347 -0.05201 0.011991 0.011397 -0.04152 -0.024969 -0.071266 -0.033222 0.009518 -0.019906 -0.040646 -0.039937 -0.050851 0.00083677 -0.018484 -0.0041713 0.0064886 -0.067036 0.0056052 -0.00054306 0.026594 0.0040021 0.012092 0.011473 0.20581 0.032891 0.027431 0.011052 0.0056615 -0.05179 -0.0076763 0.0021063 0.14283 -0.04321 0.027294 -0.0070485 -0.0040798 -0.00095276 0.051865 0.0080811 0.10058 -0.23916 -0.052258 0.058941 -0.024341 0.00013516 0.011064 -0.020483 0.082137 -0.042833 -0.035563 -0.028267 -0.0064221 0.0042652 0.040598 0.0088486 0.0061292 -0.070036 0.006802 0.012312 -0.0029997 0.012727 0.026909 -0.0055169 -0.029091 0.040185 -0.011133 0.16654 0.0090643 0.012031 0.0059387 0.012718 0.0072459 -0.014349 0.016435 -0.042527 -0.010499 -0.013 0.011871 -0.0019542 -0.011835 0.062125 -0.016075 0.16043 0.0090545 -0.0041039 0.044869 0.052771 0.025326 0.015888 0.10015 -0.022771 0.016663 -0.026813 -0.028306 -0.028848 -0.0016062 0.020304 0.11085 -0.0329 -0.2539 0.017869 -0.011855 -0.008316 -0.082658 0.041856 0.080235 0.031799 0.0058181 0.024647 0.10456 -0.085335 0.010957 0.024999 0.0096548 -0.022021 0.067772 0.0074022 0.02253 0.014399 0.011026 0.04396 -0.0056028 -0.044832 -0.017269 0.0024209 0.035486 0.0002908 -0.011592 0.0035029 0.0068229 0.030358 0.0012739 0.012805 0.0044976 0.077385 -0.049255 0.030478 -0.004851 0.079516 0.062239 -0.053378 0.0011011 0.013411 0.018024 0.01809 -0.027007 0.016819 0.018898 -0.0037581 -0.0081302 -0.0021525 0.019286 0.0078543 -0.012691 0.19913 0.019591 -0.0027062 0.014902 0.020075 0.0015501 0.026445 0.016135 0.059493 -0.053307 0.022318 -0.050047 -0.016615 -0.024473 -0.0026849 0.046809 -0.0081752 0.029239 0.084026 -0.011564 0.028693 -0.0054134 -0.085213 -0.025217 0.08599 0.013869 0.0036711 0.0091075 0.0088009 0.0074909 0.0033375 -0.036734 -0.014189 0.028213 -0.0071897 0.0090266 -0.085809 -0.022652 0.014464 0.013656 0.070779 -0.048292 0.0040565 0.47594 -0.0082463 0.022197 0.028213 -0.011865 0.036402 -0.051912 0.023115 0.031916 -0.012857 0.021043 -0.061111 0.0023336 0.0015213 -0.1093 0.006894 -0.00036093 -0.045798 -0.030809 -0.013434 0.072999 0.021477 0.01236 -0.024001 0.050199 -0.011506 -0.0030617 -0.015328 0.052906 -0.080613 0.055234 -0.055562 0.098468 -0.15044 0.033538 0.061746 0.032504 0.0040402 -0.039804 -0.027267 0.023845 -0.017978 0.017749 0.036601 -0.0068245 0.021771 -0.15036 -0.0010621 -0.035937 0.068257 -0.042055 -0.0082523 -4.2307e-05 0.0092737 -0.045102 0.005882 \n",
      "0.0015377 0.0092387 -0.0097338 -0.0035523 -0.020367 -0.017491 -0.032187 0.031876 -0.028371 -0.013964 0.0089369 -0.012344 -0.014067 0.012274 -0.0013164 -0.011613 0.0021162 0.0059631 -0.019175 0.21828 0.021152 0.023211 -0.0132 -0.035278 -0.074999 -0.026028 0.00082388 -0.0030167 -0.068258 0.063533 -0.0098843 -0.041354 0.014589 0.023406 0.014214 -0.013643 -0.20485 0.12002 -0.0040268 -0.01391 0.014301 0.016497 -0.042776 0.021333 0.087691 0.039735 0.011358 0.060788 0.0171 0.0093087 0.025502 -0.033427 -0.018634 -0.023903 -0.0083371 -0.02316 -0.042465 0.04977 -0.038526 -0.045227 -0.0066938 -0.0071283 0.0023063 -0.19473 -0.019533 -0.021869 0.010451 -0.028849 -0.005502 -0.015123 0.022984 0.0069817 0.032349 -0.0067217 0.0027925 -0.030843 0.025418 0.039041 -0.035046 -0.06629 0.0094512 -0.013102 -0.0071913 -0.0043986 0.135 -0.0076825 0.033408 0.094652 -0.014759 -0.11064 -0.010345 -0.026574 0.030875 -0.017007 -0.015609 -0.16973 -0.065682 0.0074308 -0.026655 -0.014358 -0.0026736 0.036283 0.016993 -0.064516 -0.0041457 0.012524 0.034564 0.00063491 0.018684 0.0064048 -0.013033 0.019943 0.014397 -0.024211 -0.0056503 -0.0068473 0.016352 0.053661 -0.0087903 -0.0091394 0.00057722 -0.0080925 -0.070711 -0.0040782 0.0070129 -0.020512 -0.0013618 -0.066288 0.016589 0.020321 -0.013396 -0.014969 0.043764 0.1029 -0.015092 0.00051049 -0.11061 -0.0090163 0.012812 0.020356 -0.021009 -0.028534 -0.00013742 0.014426 0.14311 0.010415 -0.32596 -0.0013444 -0.02 0.0028296 -0.0063946 0.032309 0.10178 0.023406 -0.020266 0.013604 0.090424 -0.0076669 -1.8055e-05 0.036118 0.017195 0.0059121 0.11017 0.01851 0.025506 -0.019584 -0.029501 0.0051137 -0.036162 0.014924 -0.0047672 -0.019936 0.018991 -0.026237 4.795e-05 -0.017515 0.025973 0.0065166 0.018043 0.052702 -0.012464 0.11506 -0.040293 -0.012614 0.0028379 0.051293 -0.086108 -0.049329 0.016674 0.033437 -0.014498 0.032032 0.0075133 -0.0051282 0.0043515 0.0073163 0.0029544 -0.0021679 0.0306 -0.056678 0.011125 0.20357 0.012592 -0.053368 0.035516 -0.0021093 -0.078893 0.00079947 -0.025919 -0.038966 -0.021394 -0.00037013 0.039666 0.025162 -0.0029954 -0.0096898 -0.044352 0.0021483 0.014588 -0.024184 -0.0066447 0.011702 -0.0018559 -0.0029619 0.034417 -0.052333 0.016376 -0.0024518 -0.087212 -0.037402 0.012583 0.028628 -0.19055 -0.041025 0.0021379 -0.019834 0.0098336 0.020049 -0.015907 0.0068591 -0.016352 0.11642 0.038927 -0.033593 0.38569 0.0032431 0.018101 0.016864 0.011387 -0.17907 0.042759 -0.075706 0.0028923 -0.035635 -0.0041221 0.043005 -0.038522 -0.014831 0.035471 0.026948 -0.011419 -0.0045552 -0.00048518 -0.010187 0.090232 0.01327 0.00087625 0.027611 -0.0045972 -0.0062817 -0.033988 -0.02383 -0.064154 0.071352 -0.002202 0.080456 0.044402 0.074977 0.092902 0.20613 -0.038619 -0.039551 -0.10306 0.015533 0.0086306 -0.0083894 -0.00050133 -0.015548 -0.021552 0.012502 0.12287 0.0085681 -0.072517 -0.010239 0.012317 -0.0053756 -0.00036711 0.10695 -0.016719 0.002349 \n",
      "-0.031645 -0.067418 -0.033575 -0.0051846 -0.077769 -0.014531 0.00096782 0.032046 -0.013072 -0.016324 -0.02992 0.013075 0.026857 -0.0083021 -0.057892 0.09966 0.0092115 -0.026159 -0.014019 -0.013609 -0.005136 0.037134 -0.017824 -0.010961 0.053454 -0.0064973 0.0048655 0.020265 -0.066568 0.098669 -0.031 0.0019538 -0.021089 0.011776 -0.0051042 -0.0058845 0.067536 -0.14381 0.061807 -0.035058 -0.046964 -0.0067761 -0.017991 -0.018368 0.073118 -0.017712 -0.027166 0.052068 0.030407 0.0089143 -0.010787 -0.095507 -0.026588 -0.030413 0.0013841 -0.039803 -0.019472 -0.084092 -0.026457 0.017478 0.0037656 -0.035844 -0.015722 -0.096457 -0.0085012 0.0052233 -0.018941 -0.0012207 0.012298 0.011908 0.063155 -0.030346 -0.00020445 0.0044543 -0.011221 -0.032672 0.054585 0.016977 -0.12211 0.0010163 0.02621 -0.0047975 -0.026208 0.024992 -0.085965 -0.041149 0.093917 -0.044801 0.037278 -0.0069484 -0.023098 -0.037817 0.062252 -0.038697 -0.02563 0.18043 -0.17724 0.02466 -0.026841 -0.016565 -0.00063839 0.027896 0.036709 -0.0081064 -0.031637 0.036467 0.029702 0.010041 0.0025802 0.039636 -0.0005633 0.05154 0.011939 -0.035545 0.013909 -0.020936 0.010392 0.036263 -0.010504 -0.053844 2.9983e-07 0.037842 0.040589 0.0009187 -0.0036285 -0.01873 -0.023595 -0.016715 -0.018426 0.060155 0.0049349 -0.034691 0.13029 -0.025687 0.0044656 -0.0062849 -0.18287 -0.035868 0.035171 0.042677 0.01149 0.011044 -0.039295 0.020355 -0.078262 -0.00066689 -0.30235 -0.018309 -0.0076933 -0.036969 -0.11149 -0.012308 0.036861 0.024119 0.0072186 0.018432 0.070812 -0.017184 -0.022055 0.0072829 0.017065 -0.022378 0.03934 0.0081614 0.017658 -0.0035908 -0.0074141 -0.048315 -0.011317 0.00076053 -0.027651 -0.013667 0.023784 -0.0042666 0.001361 -0.031783 -0.0015576 0.02562 -0.034145 0.015623 -0.010536 0.15263 -0.046012 -0.0061134 0.035841 0.042472 -0.070439 -0.018658 0.0075753 0.034078 -0.032387 -0.096667 -0.024841 0.013423 0.032871 0.013414 -0.0039023 -0.037506 0.010504 0.0012665 -0.015437 0.094933 0.011772 0.013597 0.049307 0.024622 0.0058101 -0.011312 -0.037035 -0.0053296 0.0034064 0.026962 -0.12838 0.054537 -0.012569 -0.023263 0.0060444 -0.0042536 0.021758 0.0050341 0.056818 -0.016255 -0.00044157 0.060068 0.028485 -0.047338 0.054356 0.021086 0.028724 0.0010327 0.011323 0.00019532 0.099374 -0.02094 -0.0015251 -0.091733 0.0080634 -0.038634 -0.034671 0.0054171 0.00888 -0.0058243 0.077075 0.0027592 0.43148 -0.051825 0.029026 0.037679 -0.080402 0.047468 -0.0076842 -0.006872 -0.027657 -0.064752 -0.0043771 -0.017031 -0.029736 -0.037576 -0.050873 -0.011796 -0.0058014 0.21911 -0.033243 -0.02955 -0.033897 -0.047974 0.020396 -0.030119 0.088287 -0.074395 0.0038974 -0.06225 0.026552 -0.0074113 -0.021678 -0.077479 -0.1604 -0.05327 -0.0024661 -0.068678 0.026108 0.030404 -0.23742 -0.082187 0.022986 -0.0095126 0.058426 -0.023502 -0.0055588 0.0063215 0.019243 0.011719 -0.059033 0.045568 -0.028094 0.0078568 0.021364 -0.073858 -0.072651 0.028801 \n",
      "-0.004642 -0.083334 0.023629 -0.0081147 -0.021058 -0.012022 0.053491 0.0088438 -0.015699 0.0063386 0.003375 0.0080384 0.022835 -0.004325 0.0033066 0.012692 0.0016189 -0.021336 0.0020341 -0.087373 -0.014415 0.015928 0.031882 0.01003 -0.013861 -0.023968 -0.010219 0.011827 0.0060535 0.034185 0.037853 -0.022722 -0.0017498 -0.093172 -0.075882 -0.030392 0.0010724 0.25866 0.032114 -0.011374 -0.026876 0.034429 -0.0031979 -0.013366 -0.048258 -0.028378 -0.018881 0.055234 -0.010601 -0.0067363 -0.010446 0.020596 -0.054133 -0.012393 -0.016917 -0.091089 -0.068308 -0.066598 0.029001 -0.011575 -0.012718 -0.00012912 0.015652 -0.04296 0.001718 -0.015983 0.027766 -0.04565 -0.02121 0.0046424 -0.034501 -0.015068 0.028977 -0.0075556 -0.01386 -0.03578 0.025365 -0.001923 0.022577 -0.12622 -0.016148 -0.0045215 0.0013893 0.0090513 0.00058227 -0.025249 0.0062074 -0.0041683 -0.0087896 -0.00073717 -0.013128 -0.010763 0.033185 0.00026065 -0.12259 0.0047229 -0.16711 -0.018613 -0.010696 -0.0049714 0.022441 -0.0024658 -0.01704 -0.063422 0.0076292 0.0013604 -0.010004 0.0018826 0.024179 0.023858 0.0095349 -0.058355 0.0033834 0.17511 0.0043067 0.010978 -0.014705 0.016427 0.0072786 -0.026239 -0.01814 -0.0018037 0.0050159 -0.0064485 -0.060502 -0.028461 0.010803 -0.042157 -0.0058141 0.072472 -0.0050068 0.0067264 0.022495 -0.00010817 -0.10004 -0.019146 -0.035023 0.049813 0.00029956 0.072432 -0.026505 -0.00085354 -0.010069 -0.011111 0.081053 0.013697 -0.2714 0.0039618 -0.015622 0.017797 -0.1429 0.002039 0.079026 -0.0017404 0.0015332 0.00043251 0.17521 -0.081993 0.01868 0.020417 0.017618 -0.014525 -0.067709 0.0025434 0.010786 -0.010641 -0.0028229 -0.058573 -0.0034925 -0.041394 -0.0043289 0.0088174 0.016302 -0.026132 0.026635 0.014674 -0.0070017 0.11253 0.0017758 -0.028481 0.035161 0.043507 -0.016345 0.095137 -0.018014 0.076895 -0.025459 -0.029947 0.037165 0.033324 -0.036169 0.011259 -0.042125 0.0017757 0.0071502 -0.0027534 -0.020526 0.025357 -0.042226 -0.023806 0.0027406 0.2201 -0.0055476 0.015368 0.0020897 -0.002462 -0.0088384 -0.0026383 0.015936 -0.087757 0.05723 0.035018 0.0079871 0.035969 -0.02257 -0.0037654 0.040565 -0.023316 -0.038478 0.060298 -0.008625 0.035541 0.027084 -0.023667 -0.074065 0.022587 0.019347 0.012515 0.082973 -0.021683 0.012118 0.004754 -0.039883 0.017792 0.010895 0.050549 0.002339 -0.039531 0.0022015 -0.014929 -0.0088301 0.22685 0.013018 -0.020141 0.45296 -0.013587 0.00066438 0.035167 0.039083 -0.13369 0.014615 0.03428 0.024015 -0.027176 0.0056762 -0.026264 -0.026458 -0.009339 -0.033494 0.050249 0.012013 -0.060192 -0.043855 -0.037103 -0.0067188 -0.017627 -0.019906 0.02614 0.10352 0.10058 -0.031418 -0.039564 0.056768 0.086873 -0.044542 -0.10561 0.065429 -0.15438 0.040658 -0.17038 0.015563 -0.070061 -0.057571 -0.039804 0.00079867 -0.014347 -0.011357 -0.0046626 -0.017151 0.029062 0.034751 0.026898 -0.010675 0.032168 -0.0034654 0.003083 -0.0064799 0.045288 0.0014906 0.01088 \n",
      "0.0012932 -0.084147 -0.031195 -0.029117 -0.0020492 -0.0052287 -0.062614 0.0076459 -0.01145 -0.026315 0.013354 -0.013715 0.084686 -0.0026761 -0.03573 0.1076 0.028456 -0.032966 -0.021117 0.0086372 0.0061703 0.017051 -0.039318 -0.056726 -0.055177 -0.024515 -0.0095313 0.029405 -0.050455 0.15178 0.005984 0.0012203 0.016598 -0.027953 0.079874 -0.012714 -0.014797 -0.033698 0.025252 0.0038685 -0.028882 -0.027586 0.00057664 0.0075073 0.048177 0.029481 0.0052477 0.098307 0.010436 -0.0068995 0.017445 -0.027279 -0.039043 -0.030631 -0.061347 -0.021047 -0.026176 0.060715 -0.051481 -0.049936 -0.040972 -0.0065095 -0.0020667 -0.085947 -0.013873 0.00042783 -0.020846 -0.029243 -0.033601 -0.030611 -0.045058 0.0051517 0.017815 0.00099039 0.013129 -0.012137 0.047254 0.058108 0.028046 -0.0082493 0.025887 -0.024234 -0.0059666 0.051992 -0.04085 -0.033652 0.088896 -0.003997 0.047403 -0.062846 0.038572 -0.041467 0.10962 -0.026848 0.058325 0.087895 -0.095955 0.061496 -0.017118 -0.010445 0.03373 0.048586 0.038479 -0.020926 -0.0025603 -0.010955 0.025433 0.024854 -0.0010052 0.010951 -0.0042878 -0.049348 -0.0018883 0.021574 -0.026716 -0.044519 -0.013643 0.030442 0.027673 0.016164 -0.013699 0.0094599 0.032026 0.038415 0.060506 0.049485 -0.015527 -0.086982 -0.047558 0.012396 -0.0033076 -0.032113 0.13048 -0.097039 0.0017247 -0.071427 -0.10067 -0.011055 0.033897 0.022819 -0.060325 -0.035101 0.022789 0.037317 -0.087259 0.062513 -0.34022 0.007242 -0.013881 0.0027568 -0.03162 0.015585 -0.017758 0.085931 0.014646 -0.017713 0.063735 0.054889 0.028891 0.028902 0.010716 -0.0017492 0.057859 0.016601 -0.0072129 -0.020761 0.040253 -0.064293 -0.0059008 0.036017 -0.014578 0.011648 0.0018491 -0.039163 0.012183 -0.065934 0.019856 -0.13226 0.028838 0.014332 -0.021329 -0.023422 0.037609 0.052945 0.025717 0.09702 -0.022466 -0.048809 -0.02024 -0.033024 -0.047532 -0.022467 -0.014783 -0.044669 0.018602 0.0099139 -0.030356 -0.024349 0.035916 -0.012831 -0.036187 0.12379 0.034651 -0.048939 0.046945 0.016302 -0.13783 -0.030221 -0.014088 -0.079691 -0.042219 0.03141 -0.108 0.039974 -0.0036724 -0.042891 0.067198 0.032737 0.068247 0.0078979 0.046911 0.0018544 -0.0065699 -0.022083 0.021262 -0.072223 0.021078 0.016525 -0.080015 -0.01107 0.018278 -0.021507 -0.050609 -0.050855 3.5667e-05 -0.10993 -0.0085335 0.024294 0.023727 8.4989e-06 0.0052608 0.037283 0.034153 -0.064752 0.25692 -0.021806 -0.046755 0.089519 0.010595 0.071626 0.042499 -0.0091863 -0.022568 -0.019093 0.0088277 -0.071932 0.0050214 -0.026151 0.064776 0.0069588 0.0042807 0.075796 -0.033034 0.0076341 0.0087262 -0.05413 0.035644 -0.016442 0.028938 -0.13222 -0.025242 -0.051606 0.021115 -0.068906 0.042442 0.040034 -0.097497 -0.079937 0.0024659 -0.01367 0.0033999 0.073422 -0.186 -0.035762 0.080345 -0.03905 0.013961 -0.069594 -0.058052 0.0015973 0.023604 -0.027864 -0.18669 -0.056707 -0.043065 0.0067854 0.014566 0.36823 -0.0075426 -0.036852 \n",
      "0.000767 -0.036357 -0.042976 0.0028124 0.03629 0.0080481 -0.13384 0.061129 0.0070955 0.01415 -0.017662 -0.041432 0.038649 -0.022184 0.036162 0.060933 0.056638 0.040975 -0.054722 0.049145 0.00085307 0.018951 -0.0086865 -0.087949 0.00088316 0.0033209 -0.043374 0.022281 -0.018089 0.2284 -0.00026959 -0.062991 -0.03313 0.036543 -0.025281 -0.077879 0.043108 0.0084051 0.013931 -0.038171 -0.017264 0.0058876 -0.011674 -0.024137 0.092619 0.051739 0.029309 0.088014 -0.13976 0.040281 0.040527 -0.052994 0.043771 0.017962 -0.041337 0.0052196 0.0041118 0.036544 -0.082104 -0.030199 0.018732 0.034297 0.010201 -0.035639 -0.032122 0.016918 0.018979 -0.020503 -0.0088525 -0.055196 0.017583 -0.026705 -0.034012 -0.055838 -0.017803 0.0089583 -0.044394 -0.049012 -0.077354 0.031543 -0.073035 0.035898 0.06453 0.059696 -0.016031 0.0029274 -0.06436 -0.012596 -0.0090487 -0.014848 -0.05701 -0.018804 0.10806 -0.059859 0.024911 -0.06268 0.0026609 0.036124 0.0007971 -0.017489 0.037408 -0.0034192 0.019129 -0.12248 0.14931 0.012085 0.040549 -0.031641 -0.02356 0.011556 0.050842 -0.054064 -0.019639 0.051648 0.056117 0.066324 0.14714 0.0088855 -0.064463 0.024177 0.049472 -0.0085731 0.0026673 -0.041549 0.0013326 -0.0022502 0.014497 -0.0095865 -0.081937 -0.0032182 0.022933 -0.011409 0.024096 -0.036081 0.054211 -0.0058018 -0.13722 0.030718 -0.050092 0.039105 0.024796 0.049181 -0.016277 0.032062 0.072025 0.02266 -0.1758 -0.0016721 -0.010745 0.063781 0.01628 -0.072686 -0.0074593 0.033771 -0.0040548 -0.0029247 0.038295 0.020623 0.0059979 -0.06422 0.005887 0.050193 0.015534 -0.13202 0.1238 -0.017065 0.026625 -0.032458 -0.027782 0.042926 -0.067777 -0.019855 -0.10152 0.015837 -0.010319 -0.045513 0.032782 -0.042824 0.0075923 -0.012718 0.012966 0.099546 0.011526 0.065095 0.055925 0.08073 0.086469 -0.01564 0.0055385 0.0033978 -0.025464 0.076911 0.014778 -0.16576 0.04074 -0.036447 -0.024559 -0.06139 0.048832 0.036976 -0.059912 -0.030341 -0.0043745 -0.056807 -0.010866 0.06096 0.047927 -0.030694 0.030616 -0.013736 0.018563 -0.064025 -0.070028 0.022435 0.0082354 0.0017768 0.0092244 0.081638 0.059109 -0.0012408 -0.0051357 0.0069643 -0.022092 0.0083898 -0.073366 -0.06945 -0.080714 -0.062073 -0.029059 -0.017047 -0.0042326 0.068803 -0.0014607 -0.020012 0.09347 -0.025879 0.010441 0.017965 0.00097647 0.052707 0.062049 -0.058195 0.011555 -0.011799 0.25883 0.044016 -0.081916 0.03669 0.0057492 0.06877 0.059475 0.0012403 -0.075776 0.09103 0.031639 0.017522 0.068409 0.02273 -0.0011854 0.035002 -0.01235 0.066604 -0.0066759 -0.011938 -0.016134 -0.029113 -0.099818 0.01136 -0.059441 -0.085497 0.019416 -0.086166 -0.0073034 0.014769 0.040835 -0.011183 0.001026 0.039773 0.019562 -0.012442 -0.022431 -0.010489 -0.20567 -0.0089548 0.014482 0.10899 0.0070808 -0.26274 0.02489 -0.020218 0.036085 -0.047659 -0.15745 0.0038634 -0.0049063 0.02945 0.045934 0.14262 -0.11275 -0.065931 \n",
      "-0.024316 0.10093 0.063159 0.16436 -0.0098596 -0.016402 -0.045917 -0.038624 0.0078168 0.015913 -0.041978 0.00072367 -0.030461 -0.0032812 -0.047413 0.039005 0.010226 -0.019428 -0.0044954 0.011859 0.028065 0.01735 -0.01724 -0.04075 -0.0095944 -0.024026 0.019966 0.033824 -0.042875 0.0099262 -0.022239 0.011155 -0.0066098 -0.2408 -0.014698 0.018685 0.015654 0.027766 -0.014526 -0.02169 -0.010941 0.046887 0.0049483 0.0027765 0.030689 0.11873 -0.010676 0.15817 0.011467 0.0296 0.017403 -0.018808 -0.040814 -0.013879 0.028875 0.0042163 0.012122 -0.026735 -0.089226 -0.0067556 -0.01004 -0.0065134 -0.053296 0.028406 0.042341 0.050176 -0.0040829 -0.040696 0.046998 0.0004477 -0.021619 0.0030052 0.039673 -0.058371 -0.0017997 -0.00039234 0.063407 0.039573 0.088685 -0.024078 -0.052918 -0.033068 -0.0090028 0.069987 -0.0037509 -0.033515 0.046126 -0.057919 0.056173 0.25576 0.033019 0.0093637 0.073929 -0.010834 0.0070703 0.032445 -0.1069 -0.023142 -0.0083157 0.0069735 0.039402 -0.014236 0.00068778 -0.033208 -0.0037186 -0.0033701 0.055767 -0.015277 0.0015595 0.0011159 0.073213 0.029209 -0.017845 0.020714 -0.025264 -0.04933 -0.005041 0.0045859 0.0066419 0.018602 -0.055681 0.0037495 -0.002374 -0.015622 -0.0079604 0.081635 -0.012677 -0.10357 -0.02186 0.039522 -0.0062953 0.0098028 0.026124 0.14662 0.023923 -0.025794 0.00031862 -0.052266 0.0051891 -0.010374 -0.035349 0.043242 -0.015423 0.0067421 -0.10409 0.040985 -0.28606 -0.0059206 -0.00088057 -0.013947 -0.073424 0.0081679 0.047239 0.088604 -0.038708 0.034067 0.19337 0.0092482 -0.011249 0.015104 0.020004 0.043025 0.012414 -0.014821 -0.033699 0.012797 -0.0021995 0.032865 0.040453 -0.036512 0.016736 0.002885 0.019729 -0.033361 -0.01159 -0.0057356 -0.058362 0.0062706 -0.013322 0.0041882 0.0072939 0.077113 -0.0035811 0.057839 0.056024 -0.020025 0.08744 -0.051887 0.015204 0.029261 -0.029979 0.020811 -0.0088965 -0.04127 -0.012844 0.012806 -0.070426 0.069846 0.038517 0.083288 -0.13088 0.12472 0.040057 -0.015217 -0.051154 -0.017731 0.027574 -0.038552 0.011118 0.022292 -0.064566 0.054828 0.038189 -0.0031992 0.010227 -0.012774 0.01078 -0.035326 0.032666 -0.0033844 0.068383 0.023112 0.0032812 0.049398 -0.014668 -0.090787 0.00085558 0.014483 0.14862 -0.0092635 0.013349 -0.00078617 -0.050109 -0.00896 0.011971 -0.041479 0.0017336 0.022979 -0.059901 -0.0069391 -0.039977 0.072387 -0.10718 -0.088925 0.22443 -0.017161 0.019552 0.042696 -0.024647 -0.054272 0.040426 -0.00045724 0.00092732 -0.0087598 0.026281 0.075251 -0.018126 -0.048909 0.0044435 -0.00023964 -0.043933 -0.030878 0.044237 0.086187 -0.057242 -0.021891 0.0099703 0.016443 0.032068 -0.056676 0.042816 -0.049059 0.067174 -0.064978 -0.0076353 0.051348 -0.17981 -0.089315 -0.015666 -0.11943 -0.0053712 -0.0885 -0.13857 0.020192 0.010481 -0.032713 0.019485 -0.093972 -0.023863 -0.020584 0.097033 0.021279 -0.12351 0.0036432 0.030531 0.034935 -0.011335 0.23643 -0.032778 0.030381 \n"
     ]
    }
   ],
   "source": [
    "!head EmbeddingIMDB.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zls3x2d7K1fs"
   },
   "source": [
    "Note that this is only for training, for testing we would:\n",
    "\n",
    "- If we kept the embedding as is, we simply calculate the new embeddings for the new words and add it to our matrix.\n",
    "\n",
    "- If we retrained the embeddings, then we would either use the output that we already have if the word was in our original vocabulary, or just leave a vector of zeros for those words if it is not.\n",
    "\n",
    "fastText outputs space-separated words. We replace them with a comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "5SVWsxocKfG5"
   },
   "outputs": [],
   "source": [
    "import fileinput\n",
    "\n",
    "with fileinput.FileInput('EmbeddingIMDB.tsv', inplace=True, backup='.bak') as file:\n",
    "    for line in file:\n",
    "        print(line.replace(' ', ','), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uket1APpLG0O"
   },
   "source": [
    "We add a first line with the variable names, to be able to import it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "oMI4rtsMLCxD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Create the first line\n",
    "firstLine = ','.join(['D'+str(i) for i in np.arange(1, 301)]) + '\\n'\n",
    "\n",
    "# Open as read only. Read the file\n",
    "with open('EmbeddingIMDB.tsv', 'r') as original: \n",
    "  data = original.read()\n",
    "\n",
    "# Open to write and write the first line and the rest\n",
    "with open('EmbeddingsIMDB.csv', 'w') as modified: \n",
    "  modified.write(firstLine + data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cq-vQoKFLbca",
    "outputId": "8d50abf2-399b-4877-d5db-96438e74c53e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1,D2,D3,D4,D5,D6,D7,D8,D9,D10,D11,D12,D13,D14,D15,D16,D17,D18,D19,D20,D21,D22,D23,D24,D25,D26,D27,D28,D29,D30,D31,D32,D33,D34,D35,D36,D37,D38,D39,D40,D41,D42,D43,D44,D45,D46,D47,D48,D49,D50,D51,D52,D53,D54,D55,D56,D57,D58,D59,D60,D61,D62,D63,D64,D65,D66,D67,D68,D69,D70,D71,D72,D73,D74,D75,D76,D77,D78,D79,D80,D81,D82,D83,D84,D85,D86,D87,D88,D89,D90,D91,D92,D93,D94,D95,D96,D97,D98,D99,D100,D101,D102,D103,D104,D105,D106,D107,D108,D109,D110,D111,D112,D113,D114,D115,D116,D117,D118,D119,D120,D121,D122,D123,D124,D125,D126,D127,D128,D129,D130,D131,D132,D133,D134,D135,D136,D137,D138,D139,D140,D141,D142,D143,D144,D145,D146,D147,D148,D149,D150,D151,D152,D153,D154,D155,D156,D157,D158,D159,D160,D161,D162,D163,D164,D165,D166,D167,D168,D169,D170,D171,D172,D173,D174,D175,D176,D177,D178,D179,D180,D181,D182,D183,D184,D185,D186,D187,D188,D189,D190,D191,D192,D193,D194,D195,D196,D197,D198,D199,D200,D201,D202,D203,D204,D205,D206,D207,D208,D209,D210,D211,D212,D213,D214,D215,D216,D217,D218,D219,D220,D221,D222,D223,D224,D225,D226,D227,D228,D229,D230,D231,D232,D233,D234,D235,D236,D237,D238,D239,D240,D241,D242,D243,D244,D245,D246,D247,D248,D249,D250,D251,D252,D253,D254,D255,D256,D257,D258,D259,D260,D261,D262,D263,D264,D265,D266,D267,D268,D269,D270,D271,D272,D273,D274,D275,D276,D277,D278,D279,D280,D281,D282,D283,D284,D285,D286,D287,D288,D289,D290,D291,D292,D293,D294,D295,D296,D297,D298,D299,D300\n",
      "-0.040902,0.058467,-0.010321,0.035355,-0.027129,0.016771,0.0054634,-0.012907,-0.014288,-0.0015782,-0.080665,0.0046677,0.020279,-0.0020526,-0.046287,-0.029847,0.012894,0.011567,-0.006924,-0.013908,-0.0067564,-0.0061605,-0.014448,0.0069548,0.0010292,-0.07417,0.010987,0.011773,-0.031122,-0.023272,0.0074486,-0.019942,-0.0082534,-0.17495,-0.01807,-0.0070631,-0.025471,0.064953,0.00166,0.02227,0.0056682,-0.0072136,-0.027801,-0.014074,-0.055771,0.049818,-0.007309,-0.017649,-0.0044152,0.040688,-0.024261,0.034503,-0.0086712,-0.043835,0.0070655,-0.053175,0.008306,0.045381,0.0078419,-0.022345,0.037185,0.0041834,0.0024069,0.00056557,0.035013,0.0054505,-0.026406,0.0071596,-0.0059939,0.0052182,0.072519,0.024593,0.042916,0.02227,-0.015782,-0.026384,0.0041797,0.028758,0.1778,0.073325,-0.0096955,0.0067667,-0.047348,0.053457,0.031812,0.00092231,0.036672,-0.034541,0.0046972,0.072503,-0.032606,-0.011956,-0.018245,0.0075088,0.046505,0.022023,0.051143,-0.044863,-0.010241,0.037496,0.02796,-0.0095708,-0.0060604,-0.10324,0.010627,-0.03997,0.0087579,0.0093861,-0.017477,0.031134,0.017434,0.019394,0.003093,0.090583,0.018057,-0.036964,-0.036321,-0.014978,0.0060368,-0.023885,-0.027523,-0.022826,-0.031551,0.019329,-0.008056,0.045678,-0.030662,-0.0092351,-0.024067,0.19493,-0.0087554,0.028139,0.0036521,0.16553,-0.080795,0.026553,0.054415,-0.055961,0.021239,-0.033446,0.0060756,-0.021134,0.0057236,0.0027277,0.027719,-0.0050092,-0.35273,0.0082917,-0.0094937,-0.035486,-0.13408,0.039838,0.073489,-0.0034581,-0.0032121,0.025436,0.16019,0.04842,-0.023324,0.017987,-0.015021,0.013696,0.11703,-0.013808,-0.0099025,0.05427,0.026326,-0.023916,0.033809,0.0040515,0.018049,0.008191,0.057818,0.0062062,-0.0040138,0.042944,-0.025664,0.040433,0.022779,-0.046318,-4.8883e-06,0.038988,0.013122,-0.011292,0.028368,0.042914,-0.00036241,-0.046587,0.012793,-0.017523,-0.015724,0.018558,-0.053632,0.014115,0.0025984,0.0089949,0.037397,-0.035029,0.025535,0.015407,-0.051156,0.26783,0.05523,-0.017034,-0.019293,-0.0026699,-0.0027113,-0.049131,0.0096978,0.029661,-0.015567,0.019077,-0.069306,0.015859,-0.0048586,-0.020223,-0.015135,-0.020899,0.015032,-0.033394,0.019819,0.065204,-0.0076069,0.10178,0.049086,0.04254,0.014913,0.033354,0.14251,-0.00081,-0.025941,-0.0442,-0.012405,0.038703,0.02784,-0.033001,0.012574,-0.060568,-0.051953,0.039296,0.0080913,0.1163,-0.056167,-0.1161,0.37443,-0.013377,-0.0040209,0.012543,0.043473,-0.050154,-0.016603,0.0096603,0.02126,0.0047077,0.052486,0.0083836,-0.055752,-0.016397,-0.061999,-0.023006,-0.022399,-0.12395,-0.0311,0.0039879,0.016163,-0.002023,0.034499,0.022067,-0.031032,0.02902,-0.0032984,-0.01236,-0.057963,-0.12938,0.051565,-0.0049257,-0.051383,-0.15686,-0.032455,-0.12125,0.0015869,0.010518,-0.18687,-0.04173,-0.0033257,-0.035298,0.0088569,-0.026278,-0.043488,0.0010525,0.013374,-0.034663,-0.045676,0.017632,-0.061425,-0.034148,-0.019806,0.18736,0.00035375,-0.0033145,\n",
      "0.0077348,-0.0844,0.024902,-0.0080303,-0.057209,0.0063456,0.061239,0.0099712,-0.044617,-0.0071456,-0.0021205,0.0008528,-0.0062793,-0.021201,-0.006211,-0.068044,0.019064,0.020137,-0.047637,0.036793,0.050057,-0.0042504,-0.0017546,0.052305,0.013753,-0.053565,0.00046371,-0.0066575,0.013947,-0.049597,0.026741,-0.046588,0.020844,0.055057,0.02002,0.0074293,0.017587,0.086267,-0.026084,0.022481,0.039969,-0.0093332,-0.020981,0.080347,-0.042025,-0.013319,-0.00011912,-0.094103,-0.011673,0.055904,0.022154,0.014672,0.018237,0.0080571,0.014226,0.039062,0.018594,-0.0070044,-0.028887,-0.019764,0.025754,-0.039162,-0.033438,-0.020375,0.012944,0.025565,0.027335,0.015244,0.020964,0.029291,-0.0070682,0.0096305,0.027635,0.011824,0.068422,-0.011683,-0.041644,0.031046,-0.091607,-0.024302,0.0050185,0.01648,-0.018306,0.028198,-0.029945,0.026387,0.051451,-0.014319,-0.041018,-0.050798,0.010539,0.020287,-0.033841,-0.013599,-0.0033988,0.018373,0.011431,-0.053602,-0.025357,0.059908,0.040233,-0.057,-0.029984,0.0029986,-0.01171,-0.06695,-0.028523,-0.011452,0.019884,-0.025299,0.0076735,-0.10848,0.00071592,0.084106,-0.015493,-0.011988,-0.038615,-0.012667,0.01447,-0.026373,0.058036,-0.0034458,-0.076167,0.019477,0.019007,0.055125,-0.015522,0.1198,-0.029747,-0.0036906,-0.041256,0.0138,-0.055065,-0.017489,0.011841,-0.0050809,-0.040001,0.056861,-0.040172,-0.027813,-0.0096248,-0.01692,0.0096287,0.028173,0.019335,0.023168,-0.34639,-0.0036149,0.02557,-0.023604,-0.037656,0.023819,-0.041261,-0.0035723,-0.0001479,0.013,0.057116,0.030595,-0.081641,-0.0044445,-0.054844,-0.022819,-0.087701,-0.0027246,-0.0026837,0.018613,0.014004,-0.038361,0.016991,-0.019694,-0.020774,0.015811,0.029177,0.031326,-0.047354,0.015371,-0.02935,0.093974,0.026554,-0.079252,0.015809,0.020847,-0.058644,-0.058825,0.028877,0.02498,-0.058841,-0.036648,0.021992,-0.047323,-0.041756,0.044525,-0.042462,0.038603,0.016339,-0.057955,0.059872,-0.061871,-0.013987,0.06434,-0.068483,-0.01658,0.0042792,-0.012916,-0.015104,-0.017309,-0.092881,0.0065289,0.050984,0.1199,0.033415,-0.0025181,0.080571,0.010451,-0.0018394,0.02565,0.047077,-0.036321,0.030346,0.039179,0.049149,0.026013,-0.013542,0.040634,0.040091,-0.010066,0.037116,-0.048651,-0.031297,-0.010997,-0.026967,-0.0227,0.034727,0.042223,0.029635,-0.049478,0.011202,0.046969,0.01332,0.030298,0.024114,-0.0018712,0.052925,-0.22494,0.5818,0.008086,0.034386,-0.039591,0.069753,0.017664,-0.040556,-0.011573,-0.020411,-0.042889,0.011886,-0.044638,-0.099456,0.010319,-0.0013186,-0.0087211,0.036302,0.08837,-0.041459,0.015912,-0.066909,0.070271,0.0026919,0.087528,-0.10913,0.0099853,-0.014125,0.052365,-0.054929,-0.017938,0.021441,0.028557,0.017762,-0.029927,-0.052941,0.054509,0.0056185,0.067787,-0.14427,0.02669,0.010511,0.039197,0.033244,-0.090301,-0.00079008,0.018471,0.057005,-0.033333,-0.022362,-0.026056,-0.02443,-0.035722,-0.019811,-0.010882,-0.038611,0.034033,\n",
      "0.026715,-0.15116,-0.015197,-0.028548,-0.014381,-0.0064371,0.079975,0.0081964,-0.027421,-0.011053,-0.0093345,-0.0043393,0.010266,0.030503,-0.04643,0.19088,-0.0035011,-0.0019071,0.00032325,0.0038223,-0.018036,0.024013,-0.0045595,-0.029075,0.00083084,0.0023315,-0.0037382,-0.0011429,0.00065664,0.062964,0.017787,0.0056634,0.018533,-0.067961,0.011556,-0.013282,0.0089305,-0.11376,0.052138,-0.00094722,-0.035845,0.027098,-0.0032741,0.0088043,0.034597,0.096793,-0.010986,0.035893,-0.00096634,-0.0087211,0.025194,0.008966,-0.039655,-0.018805,-0.0057315,0.030778,-0.011655,-0.073524,-0.067617,-0.019679,-0.012626,-0.016535,0.0095236,-0.13077,-0.034837,-0.039468,-0.012611,-0.01836,0.024338,-0.017558,-0.0063015,0.019942,-0.0017593,-0.027478,-0.021737,-0.029455,0.032975,0.021377,0.13925,0.008019,-0.0031809,-0.0063087,-0.00087741,0.0056364,0.14064,-0.010896,0.056333,0.018397,0.029947,-0.050573,-0.01512,-0.031967,0.017024,-0.018417,-0.083044,0.089182,-0.060707,0.0014469,-0.018677,-0.0060007,0.0090612,0.039657,0.023986,-0.0026608,-0.0072448,0.039513,0.0022398,0.014787,-0.016954,0.0094188,-0.024414,0.019494,-0.020215,-0.054551,0.0019112,-0.021841,-0.012878,0.043814,-0.028128,-0.010027,0.025528,0.025829,0.014608,-0.0048467,-0.0060207,-0.01386,0.017244,0.096654,-0.0040192,-0.00063078,-0.00090667,-0.032155,0.06623,0.1194,-0.0018305,0.0027202,-0.030933,0.026431,0.048738,-0.017289,0.017267,-0.0067609,0.010445,0.022023,-0.034591,-0.0016597,-0.24926,0.013919,-0.011791,0.012964,-0.31466,0.0040374,-0.12843,0.031157,-0.010014,-0.010276,0.052264,-0.11334,0.0047082,0.037061,0.029238,-0.016841,0.12161,0.0080836,0.042446,-0.027803,0.020447,0.017465,-0.016007,-0.021664,0.00469,-0.021434,0.027266,-0.04141,-0.014027,-0.0139,0.011159,-0.008685,0.017792,0.014605,0.027928,0.093544,-0.048764,0.017723,-0.013866,0.057337,0.082752,-0.013154,-0.01051,-0.0054995,-0.0042343,-0.010476,0.059732,-0.0085056,0.023191,0.002557,-0.027898,-0.0063789,0.013925,-0.079261,-0.012303,0.39034,0.013746,0.0041312,0.026987,0.012475,0.084788,-0.022082,0.002718,-0.069223,-0.042352,-0.010135,-0.098354,0.0035687,-0.016624,0.00033379,0.024881,0.015196,0.034739,0.078432,0.015737,0.020636,0.014384,0.021712,0.02472,-0.046655,0.0074236,-0.016624,0.005391,0.0040413,-0.0098199,0.027246,-0.010168,-0.03437,-0.0069535,0.085879,-0.0055935,-0.0036963,-0.043878,0.0074741,0.017868,0.029881,0.027658,-0.01773,0.38271,-0.039949,-0.016775,0.049084,-0.057255,0.0035054,0.00041894,0.053613,0.0074797,-0.041262,-0.0085846,0.034395,-0.027638,0.014838,-0.036409,0.04402,-0.046052,0.003076,-0.016255,-0.041869,-0.0080933,0.014526,0.034098,-0.053501,0.023397,-0.10689,-0.05918,-0.027258,0.038108,0.067833,-0.024031,-0.061892,-0.060041,0.024911,0.039414,-0.057166,0.034299,0.016388,-0.082726,-0.042174,0.013516,-0.017975,-0.011367,-0.014439,-0.02583,0.016608,0.13067,0.04276,-0.049022,0.042938,0.0033228,-0.0018779,-0.007374,0.16303,-0.017012,-0.0050595,\n",
      "-2.3796e-05,-0.061166,-0.023149,-0.15093,6.4452e-05,0.0037489,-0.019147,0.018709,-0.0089672,-0.0011583,-0.0031905,-0.013487,0.030455,0.017617,-0.063841,-0.094792,-0.020604,0.015176,-0.005218,0.01002,-0.022128,0.030197,-0.0044208,-0.0046505,-0.080304,-0.039789,-0.009955,-0.0063664,-0.0077789,0.063158,0.027923,-0.0075391,0.037325,-0.054372,-0.018374,-0.022694,0.13541,0.031595,-0.012439,-0.017586,-0.010651,0.033728,0.048082,-0.0093211,-0.031841,-0.19096,-0.011347,-0.05201,0.011991,0.011397,-0.04152,-0.024969,-0.071266,-0.033222,0.009518,-0.019906,-0.040646,-0.039937,-0.050851,0.00083677,-0.018484,-0.0041713,0.0064886,-0.067036,0.0056052,-0.00054306,0.026594,0.0040021,0.012092,0.011473,0.20581,0.032891,0.027431,0.011052,0.0056615,-0.05179,-0.0076763,0.0021063,0.14283,-0.04321,0.027294,-0.0070485,-0.0040798,-0.00095276,0.051865,0.0080811,0.10058,-0.23916,-0.052258,0.058941,-0.024341,0.00013516,0.011064,-0.020483,0.082137,-0.042833,-0.035563,-0.028267,-0.0064221,0.0042652,0.040598,0.0088486,0.0061292,-0.070036,0.006802,0.012312,-0.0029997,0.012727,0.026909,-0.0055169,-0.029091,0.040185,-0.011133,0.16654,0.0090643,0.012031,0.0059387,0.012718,0.0072459,-0.014349,0.016435,-0.042527,-0.010499,-0.013,0.011871,-0.0019542,-0.011835,0.062125,-0.016075,0.16043,0.0090545,-0.0041039,0.044869,0.052771,0.025326,0.015888,0.10015,-0.022771,0.016663,-0.026813,-0.028306,-0.028848,-0.0016062,0.020304,0.11085,-0.0329,-0.2539,0.017869,-0.011855,-0.008316,-0.082658,0.041856,0.080235,0.031799,0.0058181,0.024647,0.10456,-0.085335,0.010957,0.024999,0.0096548,-0.022021,0.067772,0.0074022,0.02253,0.014399,0.011026,0.04396,-0.0056028,-0.044832,-0.017269,0.0024209,0.035486,0.0002908,-0.011592,0.0035029,0.0068229,0.030358,0.0012739,0.012805,0.0044976,0.077385,-0.049255,0.030478,-0.004851,0.079516,0.062239,-0.053378,0.0011011,0.013411,0.018024,0.01809,-0.027007,0.016819,0.018898,-0.0037581,-0.0081302,-0.0021525,0.019286,0.0078543,-0.012691,0.19913,0.019591,-0.0027062,0.014902,0.020075,0.0015501,0.026445,0.016135,0.059493,-0.053307,0.022318,-0.050047,-0.016615,-0.024473,-0.0026849,0.046809,-0.0081752,0.029239,0.084026,-0.011564,0.028693,-0.0054134,-0.085213,-0.025217,0.08599,0.013869,0.0036711,0.0091075,0.0088009,0.0074909,0.0033375,-0.036734,-0.014189,0.028213,-0.0071897,0.0090266,-0.085809,-0.022652,0.014464,0.013656,0.070779,-0.048292,0.0040565,0.47594,-0.0082463,0.022197,0.028213,-0.011865,0.036402,-0.051912,0.023115,0.031916,-0.012857,0.021043,-0.061111,0.0023336,0.0015213,-0.1093,0.006894,-0.00036093,-0.045798,-0.030809,-0.013434,0.072999,0.021477,0.01236,-0.024001,0.050199,-0.011506,-0.0030617,-0.015328,0.052906,-0.080613,0.055234,-0.055562,0.098468,-0.15044,0.033538,0.061746,0.032504,0.0040402,-0.039804,-0.027267,0.023845,-0.017978,0.017749,0.036601,-0.0068245,0.021771,-0.15036,-0.0010621,-0.035937,0.068257,-0.042055,-0.0082523,-4.2307e-05,0.0092737,-0.045102,0.005882,\n",
      "0.0015377,0.0092387,-0.0097338,-0.0035523,-0.020367,-0.017491,-0.032187,0.031876,-0.028371,-0.013964,0.0089369,-0.012344,-0.014067,0.012274,-0.0013164,-0.011613,0.0021162,0.0059631,-0.019175,0.21828,0.021152,0.023211,-0.0132,-0.035278,-0.074999,-0.026028,0.00082388,-0.0030167,-0.068258,0.063533,-0.0098843,-0.041354,0.014589,0.023406,0.014214,-0.013643,-0.20485,0.12002,-0.0040268,-0.01391,0.014301,0.016497,-0.042776,0.021333,0.087691,0.039735,0.011358,0.060788,0.0171,0.0093087,0.025502,-0.033427,-0.018634,-0.023903,-0.0083371,-0.02316,-0.042465,0.04977,-0.038526,-0.045227,-0.0066938,-0.0071283,0.0023063,-0.19473,-0.019533,-0.021869,0.010451,-0.028849,-0.005502,-0.015123,0.022984,0.0069817,0.032349,-0.0067217,0.0027925,-0.030843,0.025418,0.039041,-0.035046,-0.06629,0.0094512,-0.013102,-0.0071913,-0.0043986,0.135,-0.0076825,0.033408,0.094652,-0.014759,-0.11064,-0.010345,-0.026574,0.030875,-0.017007,-0.015609,-0.16973,-0.065682,0.0074308,-0.026655,-0.014358,-0.0026736,0.036283,0.016993,-0.064516,-0.0041457,0.012524,0.034564,0.00063491,0.018684,0.0064048,-0.013033,0.019943,0.014397,-0.024211,-0.0056503,-0.0068473,0.016352,0.053661,-0.0087903,-0.0091394,0.00057722,-0.0080925,-0.070711,-0.0040782,0.0070129,-0.020512,-0.0013618,-0.066288,0.016589,0.020321,-0.013396,-0.014969,0.043764,0.1029,-0.015092,0.00051049,-0.11061,-0.0090163,0.012812,0.020356,-0.021009,-0.028534,-0.00013742,0.014426,0.14311,0.010415,-0.32596,-0.0013444,-0.02,0.0028296,-0.0063946,0.032309,0.10178,0.023406,-0.020266,0.013604,0.090424,-0.0076669,-1.8055e-05,0.036118,0.017195,0.0059121,0.11017,0.01851,0.025506,-0.019584,-0.029501,0.0051137,-0.036162,0.014924,-0.0047672,-0.019936,0.018991,-0.026237,4.795e-05,-0.017515,0.025973,0.0065166,0.018043,0.052702,-0.012464,0.11506,-0.040293,-0.012614,0.0028379,0.051293,-0.086108,-0.049329,0.016674,0.033437,-0.014498,0.032032,0.0075133,-0.0051282,0.0043515,0.0073163,0.0029544,-0.0021679,0.0306,-0.056678,0.011125,0.20357,0.012592,-0.053368,0.035516,-0.0021093,-0.078893,0.00079947,-0.025919,-0.038966,-0.021394,-0.00037013,0.039666,0.025162,-0.0029954,-0.0096898,-0.044352,0.0021483,0.014588,-0.024184,-0.0066447,0.011702,-0.0018559,-0.0029619,0.034417,-0.052333,0.016376,-0.0024518,-0.087212,-0.037402,0.012583,0.028628,-0.19055,-0.041025,0.0021379,-0.019834,0.0098336,0.020049,-0.015907,0.0068591,-0.016352,0.11642,0.038927,-0.033593,0.38569,0.0032431,0.018101,0.016864,0.011387,-0.17907,0.042759,-0.075706,0.0028923,-0.035635,-0.0041221,0.043005,-0.038522,-0.014831,0.035471,0.026948,-0.011419,-0.0045552,-0.00048518,-0.010187,0.090232,0.01327,0.00087625,0.027611,-0.0045972,-0.0062817,-0.033988,-0.02383,-0.064154,0.071352,-0.002202,0.080456,0.044402,0.074977,0.092902,0.20613,-0.038619,-0.039551,-0.10306,0.015533,0.0086306,-0.0083894,-0.00050133,-0.015548,-0.021552,0.012502,0.12287,0.0085681,-0.072517,-0.010239,0.012317,-0.0053756,-0.00036711,0.10695,-0.016719,0.002349,\n",
      "-0.031645,-0.067418,-0.033575,-0.0051846,-0.077769,-0.014531,0.00096782,0.032046,-0.013072,-0.016324,-0.02992,0.013075,0.026857,-0.0083021,-0.057892,0.09966,0.0092115,-0.026159,-0.014019,-0.013609,-0.005136,0.037134,-0.017824,-0.010961,0.053454,-0.0064973,0.0048655,0.020265,-0.066568,0.098669,-0.031,0.0019538,-0.021089,0.011776,-0.0051042,-0.0058845,0.067536,-0.14381,0.061807,-0.035058,-0.046964,-0.0067761,-0.017991,-0.018368,0.073118,-0.017712,-0.027166,0.052068,0.030407,0.0089143,-0.010787,-0.095507,-0.026588,-0.030413,0.0013841,-0.039803,-0.019472,-0.084092,-0.026457,0.017478,0.0037656,-0.035844,-0.015722,-0.096457,-0.0085012,0.0052233,-0.018941,-0.0012207,0.012298,0.011908,0.063155,-0.030346,-0.00020445,0.0044543,-0.011221,-0.032672,0.054585,0.016977,-0.12211,0.0010163,0.02621,-0.0047975,-0.026208,0.024992,-0.085965,-0.041149,0.093917,-0.044801,0.037278,-0.0069484,-0.023098,-0.037817,0.062252,-0.038697,-0.02563,0.18043,-0.17724,0.02466,-0.026841,-0.016565,-0.00063839,0.027896,0.036709,-0.0081064,-0.031637,0.036467,0.029702,0.010041,0.0025802,0.039636,-0.0005633,0.05154,0.011939,-0.035545,0.013909,-0.020936,0.010392,0.036263,-0.010504,-0.053844,2.9983e-07,0.037842,0.040589,0.0009187,-0.0036285,-0.01873,-0.023595,-0.016715,-0.018426,0.060155,0.0049349,-0.034691,0.13029,-0.025687,0.0044656,-0.0062849,-0.18287,-0.035868,0.035171,0.042677,0.01149,0.011044,-0.039295,0.020355,-0.078262,-0.00066689,-0.30235,-0.018309,-0.0076933,-0.036969,-0.11149,-0.012308,0.036861,0.024119,0.0072186,0.018432,0.070812,-0.017184,-0.022055,0.0072829,0.017065,-0.022378,0.03934,0.0081614,0.017658,-0.0035908,-0.0074141,-0.048315,-0.011317,0.00076053,-0.027651,-0.013667,0.023784,-0.0042666,0.001361,-0.031783,-0.0015576,0.02562,-0.034145,0.015623,-0.010536,0.15263,-0.046012,-0.0061134,0.035841,0.042472,-0.070439,-0.018658,0.0075753,0.034078,-0.032387,-0.096667,-0.024841,0.013423,0.032871,0.013414,-0.0039023,-0.037506,0.010504,0.0012665,-0.015437,0.094933,0.011772,0.013597,0.049307,0.024622,0.0058101,-0.011312,-0.037035,-0.0053296,0.0034064,0.026962,-0.12838,0.054537,-0.012569,-0.023263,0.0060444,-0.0042536,0.021758,0.0050341,0.056818,-0.016255,-0.00044157,0.060068,0.028485,-0.047338,0.054356,0.021086,0.028724,0.0010327,0.011323,0.00019532,0.099374,-0.02094,-0.0015251,-0.091733,0.0080634,-0.038634,-0.034671,0.0054171,0.00888,-0.0058243,0.077075,0.0027592,0.43148,-0.051825,0.029026,0.037679,-0.080402,0.047468,-0.0076842,-0.006872,-0.027657,-0.064752,-0.0043771,-0.017031,-0.029736,-0.037576,-0.050873,-0.011796,-0.0058014,0.21911,-0.033243,-0.02955,-0.033897,-0.047974,0.020396,-0.030119,0.088287,-0.074395,0.0038974,-0.06225,0.026552,-0.0074113,-0.021678,-0.077479,-0.1604,-0.05327,-0.0024661,-0.068678,0.026108,0.030404,-0.23742,-0.082187,0.022986,-0.0095126,0.058426,-0.023502,-0.0055588,0.0063215,0.019243,0.011719,-0.059033,0.045568,-0.028094,0.0078568,0.021364,-0.073858,-0.072651,0.028801,\n",
      "-0.004642,-0.083334,0.023629,-0.0081147,-0.021058,-0.012022,0.053491,0.0088438,-0.015699,0.0063386,0.003375,0.0080384,0.022835,-0.004325,0.0033066,0.012692,0.0016189,-0.021336,0.0020341,-0.087373,-0.014415,0.015928,0.031882,0.01003,-0.013861,-0.023968,-0.010219,0.011827,0.0060535,0.034185,0.037853,-0.022722,-0.0017498,-0.093172,-0.075882,-0.030392,0.0010724,0.25866,0.032114,-0.011374,-0.026876,0.034429,-0.0031979,-0.013366,-0.048258,-0.028378,-0.018881,0.055234,-0.010601,-0.0067363,-0.010446,0.020596,-0.054133,-0.012393,-0.016917,-0.091089,-0.068308,-0.066598,0.029001,-0.011575,-0.012718,-0.00012912,0.015652,-0.04296,0.001718,-0.015983,0.027766,-0.04565,-0.02121,0.0046424,-0.034501,-0.015068,0.028977,-0.0075556,-0.01386,-0.03578,0.025365,-0.001923,0.022577,-0.12622,-0.016148,-0.0045215,0.0013893,0.0090513,0.00058227,-0.025249,0.0062074,-0.0041683,-0.0087896,-0.00073717,-0.013128,-0.010763,0.033185,0.00026065,-0.12259,0.0047229,-0.16711,-0.018613,-0.010696,-0.0049714,0.022441,-0.0024658,-0.01704,-0.063422,0.0076292,0.0013604,-0.010004,0.0018826,0.024179,0.023858,0.0095349,-0.058355,0.0033834,0.17511,0.0043067,0.010978,-0.014705,0.016427,0.0072786,-0.026239,-0.01814,-0.0018037,0.0050159,-0.0064485,-0.060502,-0.028461,0.010803,-0.042157,-0.0058141,0.072472,-0.0050068,0.0067264,0.022495,-0.00010817,-0.10004,-0.019146,-0.035023,0.049813,0.00029956,0.072432,-0.026505,-0.00085354,-0.010069,-0.011111,0.081053,0.013697,-0.2714,0.0039618,-0.015622,0.017797,-0.1429,0.002039,0.079026,-0.0017404,0.0015332,0.00043251,0.17521,-0.081993,0.01868,0.020417,0.017618,-0.014525,-0.067709,0.0025434,0.010786,-0.010641,-0.0028229,-0.058573,-0.0034925,-0.041394,-0.0043289,0.0088174,0.016302,-0.026132,0.026635,0.014674,-0.0070017,0.11253,0.0017758,-0.028481,0.035161,0.043507,-0.016345,0.095137,-0.018014,0.076895,-0.025459,-0.029947,0.037165,0.033324,-0.036169,0.011259,-0.042125,0.0017757,0.0071502,-0.0027534,-0.020526,0.025357,-0.042226,-0.023806,0.0027406,0.2201,-0.0055476,0.015368,0.0020897,-0.002462,-0.0088384,-0.0026383,0.015936,-0.087757,0.05723,0.035018,0.0079871,0.035969,-0.02257,-0.0037654,0.040565,-0.023316,-0.038478,0.060298,-0.008625,0.035541,0.027084,-0.023667,-0.074065,0.022587,0.019347,0.012515,0.082973,-0.021683,0.012118,0.004754,-0.039883,0.017792,0.010895,0.050549,0.002339,-0.039531,0.0022015,-0.014929,-0.0088301,0.22685,0.013018,-0.020141,0.45296,-0.013587,0.00066438,0.035167,0.039083,-0.13369,0.014615,0.03428,0.024015,-0.027176,0.0056762,-0.026264,-0.026458,-0.009339,-0.033494,0.050249,0.012013,-0.060192,-0.043855,-0.037103,-0.0067188,-0.017627,-0.019906,0.02614,0.10352,0.10058,-0.031418,-0.039564,0.056768,0.086873,-0.044542,-0.10561,0.065429,-0.15438,0.040658,-0.17038,0.015563,-0.070061,-0.057571,-0.039804,0.00079867,-0.014347,-0.011357,-0.0046626,-0.017151,0.029062,0.034751,0.026898,-0.010675,0.032168,-0.0034654,0.003083,-0.0064799,0.045288,0.0014906,0.01088,\n",
      "0.0012932,-0.084147,-0.031195,-0.029117,-0.0020492,-0.0052287,-0.062614,0.0076459,-0.01145,-0.026315,0.013354,-0.013715,0.084686,-0.0026761,-0.03573,0.1076,0.028456,-0.032966,-0.021117,0.0086372,0.0061703,0.017051,-0.039318,-0.056726,-0.055177,-0.024515,-0.0095313,0.029405,-0.050455,0.15178,0.005984,0.0012203,0.016598,-0.027953,0.079874,-0.012714,-0.014797,-0.033698,0.025252,0.0038685,-0.028882,-0.027586,0.00057664,0.0075073,0.048177,0.029481,0.0052477,0.098307,0.010436,-0.0068995,0.017445,-0.027279,-0.039043,-0.030631,-0.061347,-0.021047,-0.026176,0.060715,-0.051481,-0.049936,-0.040972,-0.0065095,-0.0020667,-0.085947,-0.013873,0.00042783,-0.020846,-0.029243,-0.033601,-0.030611,-0.045058,0.0051517,0.017815,0.00099039,0.013129,-0.012137,0.047254,0.058108,0.028046,-0.0082493,0.025887,-0.024234,-0.0059666,0.051992,-0.04085,-0.033652,0.088896,-0.003997,0.047403,-0.062846,0.038572,-0.041467,0.10962,-0.026848,0.058325,0.087895,-0.095955,0.061496,-0.017118,-0.010445,0.03373,0.048586,0.038479,-0.020926,-0.0025603,-0.010955,0.025433,0.024854,-0.0010052,0.010951,-0.0042878,-0.049348,-0.0018883,0.021574,-0.026716,-0.044519,-0.013643,0.030442,0.027673,0.016164,-0.013699,0.0094599,0.032026,0.038415,0.060506,0.049485,-0.015527,-0.086982,-0.047558,0.012396,-0.0033076,-0.032113,0.13048,-0.097039,0.0017247,-0.071427,-0.10067,-0.011055,0.033897,0.022819,-0.060325,-0.035101,0.022789,0.037317,-0.087259,0.062513,-0.34022,0.007242,-0.013881,0.0027568,-0.03162,0.015585,-0.017758,0.085931,0.014646,-0.017713,0.063735,0.054889,0.028891,0.028902,0.010716,-0.0017492,0.057859,0.016601,-0.0072129,-0.020761,0.040253,-0.064293,-0.0059008,0.036017,-0.014578,0.011648,0.0018491,-0.039163,0.012183,-0.065934,0.019856,-0.13226,0.028838,0.014332,-0.021329,-0.023422,0.037609,0.052945,0.025717,0.09702,-0.022466,-0.048809,-0.02024,-0.033024,-0.047532,-0.022467,-0.014783,-0.044669,0.018602,0.0099139,-0.030356,-0.024349,0.035916,-0.012831,-0.036187,0.12379,0.034651,-0.048939,0.046945,0.016302,-0.13783,-0.030221,-0.014088,-0.079691,-0.042219,0.03141,-0.108,0.039974,-0.0036724,-0.042891,0.067198,0.032737,0.068247,0.0078979,0.046911,0.0018544,-0.0065699,-0.022083,0.021262,-0.072223,0.021078,0.016525,-0.080015,-0.01107,0.018278,-0.021507,-0.050609,-0.050855,3.5667e-05,-0.10993,-0.0085335,0.024294,0.023727,8.4989e-06,0.0052608,0.037283,0.034153,-0.064752,0.25692,-0.021806,-0.046755,0.089519,0.010595,0.071626,0.042499,-0.0091863,-0.022568,-0.019093,0.0088277,-0.071932,0.0050214,-0.026151,0.064776,0.0069588,0.0042807,0.075796,-0.033034,0.0076341,0.0087262,-0.05413,0.035644,-0.016442,0.028938,-0.13222,-0.025242,-0.051606,0.021115,-0.068906,0.042442,0.040034,-0.097497,-0.079937,0.0024659,-0.01367,0.0033999,0.073422,-0.186,-0.035762,0.080345,-0.03905,0.013961,-0.069594,-0.058052,0.0015973,0.023604,-0.027864,-0.18669,-0.056707,-0.043065,0.0067854,0.014566,0.36823,-0.0075426,-0.036852,\n",
      "0.000767,-0.036357,-0.042976,0.0028124,0.03629,0.0080481,-0.13384,0.061129,0.0070955,0.01415,-0.017662,-0.041432,0.038649,-0.022184,0.036162,0.060933,0.056638,0.040975,-0.054722,0.049145,0.00085307,0.018951,-0.0086865,-0.087949,0.00088316,0.0033209,-0.043374,0.022281,-0.018089,0.2284,-0.00026959,-0.062991,-0.03313,0.036543,-0.025281,-0.077879,0.043108,0.0084051,0.013931,-0.038171,-0.017264,0.0058876,-0.011674,-0.024137,0.092619,0.051739,0.029309,0.088014,-0.13976,0.040281,0.040527,-0.052994,0.043771,0.017962,-0.041337,0.0052196,0.0041118,0.036544,-0.082104,-0.030199,0.018732,0.034297,0.010201,-0.035639,-0.032122,0.016918,0.018979,-0.020503,-0.0088525,-0.055196,0.017583,-0.026705,-0.034012,-0.055838,-0.017803,0.0089583,-0.044394,-0.049012,-0.077354,0.031543,-0.073035,0.035898,0.06453,0.059696,-0.016031,0.0029274,-0.06436,-0.012596,-0.0090487,-0.014848,-0.05701,-0.018804,0.10806,-0.059859,0.024911,-0.06268,0.0026609,0.036124,0.0007971,-0.017489,0.037408,-0.0034192,0.019129,-0.12248,0.14931,0.012085,0.040549,-0.031641,-0.02356,0.011556,0.050842,-0.054064,-0.019639,0.051648,0.056117,0.066324,0.14714,0.0088855,-0.064463,0.024177,0.049472,-0.0085731,0.0026673,-0.041549,0.0013326,-0.0022502,0.014497,-0.0095865,-0.081937,-0.0032182,0.022933,-0.011409,0.024096,-0.036081,0.054211,-0.0058018,-0.13722,0.030718,-0.050092,0.039105,0.024796,0.049181,-0.016277,0.032062,0.072025,0.02266,-0.1758,-0.0016721,-0.010745,0.063781,0.01628,-0.072686,-0.0074593,0.033771,-0.0040548,-0.0029247,0.038295,0.020623,0.0059979,-0.06422,0.005887,0.050193,0.015534,-0.13202,0.1238,-0.017065,0.026625,-0.032458,-0.027782,0.042926,-0.067777,-0.019855,-0.10152,0.015837,-0.010319,-0.045513,0.032782,-0.042824,0.0075923,-0.012718,0.012966,0.099546,0.011526,0.065095,0.055925,0.08073,0.086469,-0.01564,0.0055385,0.0033978,-0.025464,0.076911,0.014778,-0.16576,0.04074,-0.036447,-0.024559,-0.06139,0.048832,0.036976,-0.059912,-0.030341,-0.0043745,-0.056807,-0.010866,0.06096,0.047927,-0.030694,0.030616,-0.013736,0.018563,-0.064025,-0.070028,0.022435,0.0082354,0.0017768,0.0092244,0.081638,0.059109,-0.0012408,-0.0051357,0.0069643,-0.022092,0.0083898,-0.073366,-0.06945,-0.080714,-0.062073,-0.029059,-0.017047,-0.0042326,0.068803,-0.0014607,-0.020012,0.09347,-0.025879,0.010441,0.017965,0.00097647,0.052707,0.062049,-0.058195,0.011555,-0.011799,0.25883,0.044016,-0.081916,0.03669,0.0057492,0.06877,0.059475,0.0012403,-0.075776,0.09103,0.031639,0.017522,0.068409,0.02273,-0.0011854,0.035002,-0.01235,0.066604,-0.0066759,-0.011938,-0.016134,-0.029113,-0.099818,0.01136,-0.059441,-0.085497,0.019416,-0.086166,-0.0073034,0.014769,0.040835,-0.011183,0.001026,0.039773,0.019562,-0.012442,-0.022431,-0.010489,-0.20567,-0.0089548,0.014482,0.10899,0.0070808,-0.26274,0.02489,-0.020218,0.036085,-0.047659,-0.15745,0.0038634,-0.0049063,0.02945,0.045934,0.14262,-0.11275,-0.065931,\n"
     ]
    }
   ],
   "source": [
    "!head EmbeddingsIMDB.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuFMUUKwLhcv"
   },
   "source": [
    "Just what we wanted! Now we have a matrix with every word in the document with its corresponding Embedding. We can now import this file into Python, and use it to train our model.\n",
    "\n",
    "## Using the Embedding Layer\n",
    "\n",
    "The next step is to actually train a neural network with an Embedding Layer. For this, Keras has the aptly named \"Embedding\" layer, which will take care of our structures. The following code creates a very simple network that does the following:\n",
    "\n",
    "1. Read the embeddings.\n",
    "2. Calculate the One-Hot inputs (by using an \"index\") which will index which words are in which text.\n",
    "3. Create a layer that associates the indexes with the embeddings.\n",
    "4. Create the rest of the architecture.\n",
    "5. Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "8-VLJQqOLftz",
    "outputId": "a4ae2a01-5d22-482d-ef35-e978c20c361b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-f98b4f8b-4461-4e08-a006-fd095f8affac\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>D10</th>\n",
       "      <th>...</th>\n",
       "      <th>D291</th>\n",
       "      <th>D292</th>\n",
       "      <th>D293</th>\n",
       "      <th>D294</th>\n",
       "      <th>D295</th>\n",
       "      <th>D296</th>\n",
       "      <th>D297</th>\n",
       "      <th>D298</th>\n",
       "      <th>D299</th>\n",
       "      <th>D300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>18184.000000</td>\n",
       "      <td>18184.000000</td>\n",
       "      <td>18184.000000</td>\n",
       "      <td>18184.000000</td>\n",
       "      <td>18184.000000</td>\n",
       "      <td>18184.000000</td>\n",
       "      <td>18184.000000</td>\n",
       "      <td>18184.000000</td>\n",
       "      <td>18184.000000</td>\n",
       "      <td>18184.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>18184.000000</td>\n",
       "      <td>18184.000000</td>\n",
       "      <td>18184.000000</td>\n",
       "      <td>18184.000000</td>\n",
       "      <td>18184.000000</td>\n",
       "      <td>18184.000000</td>\n",
       "      <td>18184.000000</td>\n",
       "      <td>18184.000000</td>\n",
       "      <td>18184.000000</td>\n",
       "      <td>18184.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.008114</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>-0.002125</td>\n",
       "      <td>0.061677</td>\n",
       "      <td>-0.013451</td>\n",
       "      <td>-0.015525</td>\n",
       "      <td>0.011203</td>\n",
       "      <td>-0.010970</td>\n",
       "      <td>-0.003307</td>\n",
       "      <td>0.007139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012734</td>\n",
       "      <td>-0.004633</td>\n",
       "      <td>-0.065000</td>\n",
       "      <td>0.018106</td>\n",
       "      <td>0.007834</td>\n",
       "      <td>0.002493</td>\n",
       "      <td>-0.003404</td>\n",
       "      <td>0.059532</td>\n",
       "      <td>-0.002828</td>\n",
       "      <td>-0.006115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.053542</td>\n",
       "      <td>0.054069</td>\n",
       "      <td>0.056607</td>\n",
       "      <td>0.053417</td>\n",
       "      <td>0.058679</td>\n",
       "      <td>0.060263</td>\n",
       "      <td>0.056204</td>\n",
       "      <td>0.051736</td>\n",
       "      <td>0.053164</td>\n",
       "      <td>0.052639</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062315</td>\n",
       "      <td>0.054321</td>\n",
       "      <td>0.063343</td>\n",
       "      <td>0.057616</td>\n",
       "      <td>0.052185</td>\n",
       "      <td>0.052340</td>\n",
       "      <td>0.050751</td>\n",
       "      <td>0.058838</td>\n",
       "      <td>0.057073</td>\n",
       "      <td>0.057053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.238420</td>\n",
       "      <td>-0.258440</td>\n",
       "      <td>-0.234590</td>\n",
       "      <td>-0.209000</td>\n",
       "      <td>-0.269920</td>\n",
       "      <td>-0.252730</td>\n",
       "      <td>-0.241330</td>\n",
       "      <td>-0.225070</td>\n",
       "      <td>-0.253600</td>\n",
       "      <td>-0.192840</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.286190</td>\n",
       "      <td>-0.260210</td>\n",
       "      <td>-0.372480</td>\n",
       "      <td>-0.216090</td>\n",
       "      <td>-0.205250</td>\n",
       "      <td>-0.231730</td>\n",
       "      <td>-0.195230</td>\n",
       "      <td>-0.182810</td>\n",
       "      <td>-0.218960</td>\n",
       "      <td>-0.242250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.043690</td>\n",
       "      <td>-0.032514</td>\n",
       "      <td>-0.039860</td>\n",
       "      <td>0.029557</td>\n",
       "      <td>-0.051335</td>\n",
       "      <td>-0.054479</td>\n",
       "      <td>-0.025823</td>\n",
       "      <td>-0.044332</td>\n",
       "      <td>-0.038836</td>\n",
       "      <td>-0.027946</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029086</td>\n",
       "      <td>-0.040105</td>\n",
       "      <td>-0.091571</td>\n",
       "      <td>-0.021151</td>\n",
       "      <td>-0.027146</td>\n",
       "      <td>-0.031749</td>\n",
       "      <td>-0.036911</td>\n",
       "      <td>0.025305</td>\n",
       "      <td>-0.040890</td>\n",
       "      <td>-0.043635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.008481</td>\n",
       "      <td>0.001711</td>\n",
       "      <td>-0.002530</td>\n",
       "      <td>0.056534</td>\n",
       "      <td>-0.012240</td>\n",
       "      <td>-0.015622</td>\n",
       "      <td>0.011373</td>\n",
       "      <td>-0.011645</td>\n",
       "      <td>-0.004031</td>\n",
       "      <td>0.006403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010355</td>\n",
       "      <td>-0.003896</td>\n",
       "      <td>-0.056295</td>\n",
       "      <td>0.016616</td>\n",
       "      <td>0.008979</td>\n",
       "      <td>0.002177</td>\n",
       "      <td>-0.002744</td>\n",
       "      <td>0.065219</td>\n",
       "      <td>-0.002057</td>\n",
       "      <td>-0.006468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.027115</td>\n",
       "      <td>0.035524</td>\n",
       "      <td>0.035258</td>\n",
       "      <td>0.089126</td>\n",
       "      <td>0.026632</td>\n",
       "      <td>0.025418</td>\n",
       "      <td>0.048343</td>\n",
       "      <td>0.022532</td>\n",
       "      <td>0.030916</td>\n",
       "      <td>0.041714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052027</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>-0.025976</td>\n",
       "      <td>0.055056</td>\n",
       "      <td>0.043000</td>\n",
       "      <td>0.036707</td>\n",
       "      <td>0.029690</td>\n",
       "      <td>0.093998</td>\n",
       "      <td>0.035636</td>\n",
       "      <td>0.031268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.206840</td>\n",
       "      <td>0.262040</td>\n",
       "      <td>0.275430</td>\n",
       "      <td>0.451850</td>\n",
       "      <td>0.216590</td>\n",
       "      <td>0.198930</td>\n",
       "      <td>0.260180</td>\n",
       "      <td>0.194320</td>\n",
       "      <td>0.218730</td>\n",
       "      <td>0.255010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.315650</td>\n",
       "      <td>0.210360</td>\n",
       "      <td>0.208360</td>\n",
       "      <td>0.240600</td>\n",
       "      <td>0.189180</td>\n",
       "      <td>0.243320</td>\n",
       "      <td>0.193750</td>\n",
       "      <td>0.460820</td>\n",
       "      <td>0.261510</td>\n",
       "      <td>0.217310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 300 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f98b4f8b-4461-4e08-a006-fd095f8affac')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-f98b4f8b-4461-4e08-a006-fd095f8affac button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-f98b4f8b-4461-4e08-a006-fd095f8affac');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                 D1            D2            D3            D4            D5  \\\n",
       "count  18184.000000  18184.000000  18184.000000  18184.000000  18184.000000   \n",
       "mean      -0.008114      0.001208     -0.002125      0.061677     -0.013451   \n",
       "std        0.053542      0.054069      0.056607      0.053417      0.058679   \n",
       "min       -0.238420     -0.258440     -0.234590     -0.209000     -0.269920   \n",
       "25%       -0.043690     -0.032514     -0.039860      0.029557     -0.051335   \n",
       "50%       -0.008481      0.001711     -0.002530      0.056534     -0.012240   \n",
       "75%        0.027115      0.035524      0.035258      0.089126      0.026632   \n",
       "max        0.206840      0.262040      0.275430      0.451850      0.216590   \n",
       "\n",
       "                 D6            D7            D8            D9           D10  \\\n",
       "count  18184.000000  18184.000000  18184.000000  18184.000000  18184.000000   \n",
       "mean      -0.015525      0.011203     -0.010970     -0.003307      0.007139   \n",
       "std        0.060263      0.056204      0.051736      0.053164      0.052639   \n",
       "min       -0.252730     -0.241330     -0.225070     -0.253600     -0.192840   \n",
       "25%       -0.054479     -0.025823     -0.044332     -0.038836     -0.027946   \n",
       "50%       -0.015622      0.011373     -0.011645     -0.004031      0.006403   \n",
       "75%        0.025418      0.048343      0.022532      0.030916      0.041714   \n",
       "max        0.198930      0.260180      0.194320      0.218730      0.255010   \n",
       "\n",
       "       ...          D291          D292          D293          D294  \\\n",
       "count  ...  18184.000000  18184.000000  18184.000000  18184.000000   \n",
       "mean   ...      0.012734     -0.004633     -0.065000      0.018106   \n",
       "std    ...      0.062315      0.054321      0.063343      0.057616   \n",
       "min    ...     -0.286190     -0.260210     -0.372480     -0.216090   \n",
       "25%    ...     -0.029086     -0.040105     -0.091571     -0.021151   \n",
       "50%    ...      0.010355     -0.003896     -0.056295      0.016616   \n",
       "75%    ...      0.052027      0.032258     -0.025976      0.055056   \n",
       "max    ...      0.315650      0.210360      0.208360      0.240600   \n",
       "\n",
       "               D295          D296          D297          D298          D299  \\\n",
       "count  18184.000000  18184.000000  18184.000000  18184.000000  18184.000000   \n",
       "mean       0.007834      0.002493     -0.003404      0.059532     -0.002828   \n",
       "std        0.052185      0.052340      0.050751      0.058838      0.057073   \n",
       "min       -0.205250     -0.231730     -0.195230     -0.182810     -0.218960   \n",
       "25%       -0.027146     -0.031749     -0.036911      0.025305     -0.040890   \n",
       "50%        0.008979      0.002177     -0.002744      0.065219     -0.002057   \n",
       "75%        0.043000      0.036707      0.029690      0.093998      0.035636   \n",
       "max        0.189180      0.243320      0.193750      0.460820      0.261510   \n",
       "\n",
       "               D300  \n",
       "count  18184.000000  \n",
       "mean      -0.006115  \n",
       "std        0.057053  \n",
       "min       -0.242250  \n",
       "25%       -0.043635  \n",
       "50%       -0.006468  \n",
       "75%        0.031268  \n",
       "max        0.217310  \n",
       "\n",
       "[8 rows x 300 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read word embeddings\n",
    "Embeddings = pd.read_csv('EmbeddingsIMDB.csv', sep=',', decimal = '.', \n",
    "                         low_memory = True, index_col = False)\n",
    "Embeddings.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRdxmBYFL221"
   },
   "source": [
    "We will now create a dictionary for the embeddings. The zip function allows to create the (key, element) structure that we need. Read more about the zip function [here](https://docs.python.org/3.7/library/functions.html#zip). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "3euIHnaCLydM"
   },
   "outputs": [],
   "source": [
    "# Create embedding dictionary\n",
    "\n",
    "EmbeddingsDict = dict(zip(Vals, Embeddings.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJ60UOCiL-vB"
   },
   "source": [
    "Now, let's study our texts to create the optimal embedding layer. One of the decisions we need to make is what is going to be the maximum size of our documents. Too large, and we will need to add a lot of padding thus will make it inefficient; too small, and we will be losing a lot of information. There is no clear rule here, I usually try to cover 90% of all elements, but you can argue anything that makes sense to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JEHS-AiL8o7"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# Count maximum number of words per file.\n",
    "wordDist = [len(w.split()) for w in texts.iloc[:,0]]\n",
    "print('Avg. no of words: ' + str(np.round(np.mean(wordDist), 2)))\n",
    "print('Std. deviation: ' + str(np.round(np.std(wordDist), 2)))\n",
    "print('Max words: ' + str(np.max(wordDist)))\n",
    "\n",
    "# Generate the plot\n",
    "distIMDB = sns.distplot(wordDist)\n",
    "\n",
    "# I'm saving the image to a PDF, as it makes it easier later to download.\n",
    "distIMDB.figure.savefig(\"wordDist.pdf\", format = \"pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9ESSnPPMRpZ"
   },
   "source": [
    "Arbitrarily, we will use 600 words maximum. Try different values!\n",
    "\n",
    "Now we create the input layer. The first layer will have the index of each word per-text, which then we will use to efficiently associate with the embedding. For this, we use Keras' \"pad_sequence\". This will either add padding to texts that are smaller than 600, or trim the ones that are longer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TQa-FIU-MKrk"
   },
   "outputs": [],
   "source": [
    "# Create word index from input\n",
    "sequences = tokenizer.texts_to_sequences(texts.iloc[:,0]) # Create the sequences.\n",
    "\n",
    "# Creates the indexes. Word index is a dictionary with words in it.\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# Creates the training dataset, adding padding when necessary.\n",
    "data = pad_sequences(sequences, maxlen=600, \n",
    "                     padding = 'post') # add padding at the end. No difference in practice.\n",
    "\n",
    "# Creates the objective function\n",
    "labels = texts.iloc[:,1]\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ptXIANqaMhWq"
   },
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iB8hcqj3MkCT"
   },
   "source": [
    "As we can see above, our data now is a matrix corresponding to where on the embedding matrix is the vector we are looking for. This is an extremely efficient way of storing embeddings, but uses more CPU. That's ok though!\n",
    "\n",
    "Now we are almost ready! Now we need to construct the Embedding matrix. This matrix will have the weights associated with each index. Keras will automatically construct the correct embedding of length 600 (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V86-BaoCMixC"
   },
   "outputs": [],
   "source": [
    "# Create first matrix full with 0's\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "\n",
    "# Generate embeddings matrix\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = EmbeddingsDict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Print what came out\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "548hhdrfRXm-"
   },
   "source": [
    "## Modelling using an embedding layer\n",
    "\n",
    "Now that we have this ready, we need to create our model and add an [Embedding Layer](https://keras.io/layers/embeddings/). We'll create a very simple model using Convolutional Layers as hidden layers. In the next lecture we'll check in detail what this means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "73fUd6CDRWz-"
   },
   "outputs": [],
   "source": [
    "# Final model.\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(len(word_index) + 1,           # Words in the embedding.\n",
    "                            300,                           # Embedding dimension\n",
    "                            weights=[embedding_matrix],    # The weights we just calculated\n",
    "                            input_length=600,              # The maximum number of words.\n",
    "                            trainable=False)               # To NOT recalculate weights!\n",
    "\n",
    "model.add(embedding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQYe-xGlR-_r"
   },
   "source": [
    "Very important: If you are letting your embedding to adapt to your own model, you need to set \"trainable=True\", if not, leave to False.\n",
    "\n",
    "Done! We have a model that uses an embedding layer as input. Let's try it in a (very bad) model.Our network will take the embedding as input and will estimate the probability of being of class 1 or 0 (positive or negative). A potential architecture is as follows:\n",
    "\n",
    "- A [1D-Convolutional Layer](https://keras.io/layers/convolutional/): See the next lecture for details :). I will add 64 filters and a kernel size of 3, which means \"look for 64 different combinations of 3 words that are useful\". We use ReLU activation for it.\n",
    "\n",
    "- A [Flatten](https://keras.io/layers/core/#flatten) layer: The embedding matrix comes as a, well, a matrix, the output of the first layer will be as well. We need to change this to a shallow 1D tensor. The Flatten layer takes matrices (or N-Dimensional tensors) and turns them into 1D tensors.\n",
    "\n",
    "- A Dense layer with 64 neurons and ReLU activation.\n",
    "\n",
    "- A [Dropout](https://keras.io/layers/core/#dropout) layer: Big models can have many millions of parameters. These models are prone to be overfitted. [Srivastava et al. (2014)](http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf) realized that a simple way to avoid overadjustment was to simply randomly set a large number of parameters to 0. This is called \"Dropout\". We will randomly set 40% of all weights to 0. This is a tunable parameter, you should experiment with parameters that make sense to you.\n",
    "\n",
    "- A sigmoid output layer, with 1 neuron. As this is a binary problem, that's the most appropriate one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Gf6bNhnPRSAE"
   },
   "outputs": [],
   "source": [
    "# Check for 64 sequences of length 3.\n",
    "model.add(Conv1D(64, 3, activation = 'relu'))\n",
    "\n",
    "# Turn output matrices into 1D tensor for shallow network.\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add 64 neurons with ReLU activation.\n",
    "model.add(Dense(64))\n",
    "\n",
    "# Add dropout.\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Add an output layer with a sigmoid.\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAF8pfDvTjZM"
   },
   "source": [
    "As this is a binary classification problem, we need a binary cross-entropy error function. I will use the optimizer [Adam](https://arxiv.org/abs/1412.6980) by Kingman et al. (2014), which works well for this problem and, more importantly, requires little tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "SE0plUHMTj0y"
   },
   "outputs": [],
   "source": [
    "# Use Adam as optimizer, with a binary_crossentropy error.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhVjsP7JTsJL"
   },
   "source": [
    "Done! Let's see the architecture of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o3QrRLNqTlbD",
    "outputId": "6c89a591-e3db-4588-e52a-3d0c535c08b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 600, 300)          5455500   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 598, 64)           57664     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 38272)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                2449472   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " activation (Activation)     (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,962,701\n",
      "Trainable params: 2,507,201\n",
      "Non-trainable params: 5,455,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dutuCWHcTvws"
   },
   "source": [
    "The model has around 2.5 million trainable parameters, the rest come from the embedding, which we left unchanged. Quite an increase compared to our last model!\n",
    "\n",
    "Now we train. We will use 33% of the data as a test set, and train for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XHIq1ib4Ttrz"
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "history = model.fit(data, labels, validation_split=0.33, epochs=30, batch_size=20)\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCqHmoyvbObP"
   },
   "source": [
    "We have clear signs the model overadjusted, but this is to be expected in data so simple. That's it! Now we can garnish the power of embeddings for modelling. We just need to learn to create models that can leverage this power. Remember, fastText is available for several hundreds of languages, so they can be used in many contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeZukXEyb6i2"
   },
   "source": [
    "### Other other embeddings\n",
    "\n",
    "- GloVe: Keras has its own tutorial [here](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html).\n",
    "\n",
    "- BERT: This one, as is so new, is far more advanced. You need to install a few new packages to make it work. Read [this](https://github.com/hanxiao/bert-as-service) or [this](https://pypi.org/project/keras-bert/).\n",
    "\n",
    "### Categorical Embeddings\n",
    "\n",
    "Additionally, embeddings can also be used to efficiently encode categorical variables. [Read this](https://towardsdatascience.com/deep-embeddings-for-categorical-variables-cat2vec-b05c8ab63ac0)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Word_Embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
