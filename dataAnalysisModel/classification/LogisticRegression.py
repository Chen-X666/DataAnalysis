# _*_ coding: utf-8 _*_
"""
Time:     2021/7/20 14:29
Author:   ChenXin
Version:  V 0.1
File:     LogisticRegression.py
Describe:  Github link: https://github.com/Chen-X666
"""
import seaborn as sns
import numpy as np # numpyåº“
import pandas as pd # pandasåº“
import prettytable # å›¾è¡¨æ‰“å°å·¥å…·
from sklearn import metrics
import matplotlib.pyplot as plt  # ç”»å›¾å·¥å…·
from sklearn import tree, linear_model  # æ ‘ã€çº¿æ€§æ¨¡å‹
from sklearn.model_selection import train_test_split # æ•°æ®åˆ‡å‰²
from sklearn.model_selection import GridSearchCV  # ç½‘æ ¼æœç´¢
from sklearn.metrics import accuracy_score, auc, confusion_matrix, f1_score, precision_score, recall_score, roc_curve, \
    roc_auc_score  # åˆ†ç±»æŒ‡æ ‡åº“

from dataAnalysisModel.classification.classificationMetrics import ConfusionMatrix, ROC, valueEvaluation
# é€»è¾‘å›å½’å®éªŒ
#***********************************é€»è¾‘å›å½’å®éªŒ**********************************************
from dataAnalysisModelEvaluation.learningLine import plot_learning_curve



def LogisticRegress(X_train, X_test, y_train, y_test):
    print('='*20+"é€»è¾‘å›å½’å®ç°"+'='*20)
    # è®­ç»ƒé›†å’ŒéªŒè¯é›†4ï¼š1åˆ‡åˆ†
    #X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.2,random_state=0)

    #æŸ¥çœ‹æ€»æ ·æœ¬é‡ã€æ€»ç‰¹å¾æ•°
    n_samples,n_features=X_train.shape
    print('{:-^60}'.format('æ ·æœ¬æ•°æ®å®¡æŸ¥'))
    print('æ ·æœ¬é‡: {0} | ç‰¹å¾æ•°: {1}'.format(n_samples,n_features))

    # è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹
    parameters = {'penalty':['l1','l2'],'C':[0.001,0.01,0.1,1,10,100],'solver':['lbfgs']} # å¯ä¼˜åŒ–å‚æ•°
    parameters = {'penalty': ['l2'], 'C': [0.1], 'solver': ['lbfgs']}  # æœ€ä¼˜å‚æ•°
    model_t = GridSearchCV(estimator=linear_model.LogisticRegression(max_iter=1000), param_grid=parameters,verbose=0,cv=10,n_jobs=-1,scoring='roc_auc')  # å»ºç«‹äº¤å‰æ£€éªŒæ¨¡å‹å¯¹è±¡ï¼Œå¹¶è¡Œæ•°ä¸CPUä¸€è‡´
    model_t.fit(X_train, y_train)  # è®­ç»ƒäº¤å‰æ£€éªŒæ¨¡å‹
    print('{:-^60}'.format('æ¨¡å‹æœ€ä¼˜åŒ–å‚æ•°'))
    print('æœ€ä¼˜å¾—åˆ†:', model_t.best_score_)  # è·å¾—äº¤å‰æ£€éªŒæ¨¡å‹å¾—å‡ºçš„æœ€ä¼˜å¾—åˆ†,é»˜è®¤æ˜¯Ræ–¹
    print('æœ€ä¼˜å‚æ•°:', model_t.best_params_)  # è·å¾—äº¤å‰æ£€éªŒæ¨¡å‹å¾—å‡ºçš„æœ€ä¼˜å‚æ•°
    model = model_t.best_estimator_  # è·å¾—äº¤å‰æ£€éªŒæ¨¡å‹å¾—å‡ºçš„æœ€ä¼˜æ¨¡å‹å¯¹è±¡
    model.fit(X_train, y_train)  # æ‹Ÿåˆæœ€ä¼˜å‚æ•°çš„æ¨¡å‹

    # é¢„æµ‹
    y_predict = model.predict(X_test)  #
    y_predict_df = pd.DataFrame(y_predict, columns=['y_predict'], index=y_test.index)
    y_test_predict_df = pd.concat([y_test, y_predict_df], axis=1)
    y_score = model.predict_proba(X_test)  # è·å¾—å†³ç­–æ ‘å¯¹æ¯ä¸ªæ ·æœ¬ç‚¹çš„é¢„æµ‹æ¦‚ç‡
    print('çœŸå®å€¼ä¸é¢„æµ‹å€¼', '-' * 30, '\n', y_test_predict_df)


    # åˆæ­¥è¯„ä»·
    print('æ¨¡å‹','-'*30,'\n',model)
    print('æ¨¡å‹ç³»æ•°','-'*30,'\n',model.coef_)
    print('æ¨¡å‹æˆªè·','-'*30,'\n',model.intercept_)
    print('æ¨¡å‹å¾—åˆ†','-'*30,'\n',model.score(X_test,y_test))

    print(y_test)
    print(y_score)
    # # è¯„ä¼°æ¨¡å‹
    ROC(modelName='Logistic Regression ROC',y_test=y_test, y_score=y_score)
    #
    # # ç‰¹å¾é‡è¦æ€§
    # featureImportant(X_train,model)
    #
    # # æ··æ·†çŸ©é˜µ
    ConfusionMatrix(y_test, y_predict)
    #
    # # æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡ï¼šaccuracyï¼Œprecisionï¼Œrecallï¼Œf1åˆ†æ•°
    valueEvaluation(y_test,y_predict,y_score)
    #
    # # å­¦ä¹ æ›²çº¿
    # plot_learning_curve(model, X_train, X_test, y_train, y_test)

    return model

def featureImportant(X,model):
    sns.set()
    coef_lr = pd.DataFrame({'var': X.columns,
                            'coef': model.coef_.flatten()
                            })

    index_sort = np.abs(coef_lr['coef']).sort_values().index
    coef_lr_sort = coef_lr.loc[index_sort, :]
    plt.bar(coef_lr_sort['var'], coef_lr_sort['coef'])  # ç”»å‡ºæ¡å½¢å›¾
    plt.title('feature importance')  # ç½‘æ ¼æ ‡é¢˜
    plt.xlabel('features')  # xè½´æ ‡é¢˜
    plt.ylabel('importance')  # yè½´æ ‡é¢˜
    print((coef_lr_sort['coef']).to_list())
    plt.xticks(range(len((coef_lr_sort['var']).to_list())),(coef_lr_sort['var']).to_list(), rotation=10, size=8)
    #æ˜¾ç¤ºæ•°å­—
    for a, b in zip(coef_lr_sort['var'], coef_lr_sort['coef']):
        plt.text(a, b, '%.3f' % b, ha='center', va='bottom', fontsize=8)
    plt.show()  # å±•ç¤ºå›¾å½¢
    print(coef_lr_sort)


# Penalty
# 'l1' penalty refers to LASSO regression (more strict), and 'l2' to Ridge regression (less strict, but easiear to solve). My advice: Start with LASSO, if it doesn't work or you are not happy with the results, move to Ridge. Large problems (over 10 million points) might require the use of Ridge regression in home computers.
# Penalty constant
# This refers to how much to weight the error in prediction versus the regularization (penalty) error. When optimizing the parameters, a penalization constant will try to optimise the following:
# ğ¸ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ=ğ¸ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘›+ğ¶Ã—ğ¸ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿğ‘ğ‘’ğ‘›ğ‘ğ‘™ğ‘¡ğ‘¦
# So the
# ğ¶
# constant will balance both objectives. This is a tunable parameter, meaning we need to search for what leaves us happy. In general, if you think the model is being too strict, then reduce C; if it is being too lax, increase it.
#
# Class weighting
# Most interesting problems are unbalanced. This means the interesting class (Default in our case) has less cases than the opposite class. Models optimise the sum over all cases, so if we minimize the error, which class do you think will be better classified?
# This means we need to balance the classes to make them equal. Luckily for us, Scikit-Learn includes automatic weighting that assigns the same error to both classes. The error becomes the following:
# ğ¸ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ=ğ‘Šğ‘’ğ‘–ğ‘”â„ğ‘¡1Ã—ğ¸ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ¶ğ‘™ğ‘ğ‘ ğ‘ 1+ğ‘Šğ‘’ğ‘–ğ‘”â„ğ‘¡2Ã—ğ¸ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ¶ğ‘™ğ‘ğ‘ ğ‘ 2+ğ¶Ã—ğ¸ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿğ‘ğ‘’ğ‘›ğ‘ğ‘™ğ‘¡ğ‘¦
# The weights are selected so the theoretical maximum error in both classes is the same (see the help for the exact equation).
#
# Random State
# The random seed. Remember to use your student ID.
#
# Iterations
# The solution comes from an iterative model, thus we specify a maximum number of iterations. Remember to check for convergence after it has been solved!
#
# Solver
# Data science functions are complex ones, with thousands, millions, or even billions of parameters. Thus we need to use the best possible solver for our problems. Several are implemented in scikit-learn. The help states that:
# For small datasets, â€˜liblinearâ€™ is a good choice, whereas â€˜sagâ€™ and â€˜sagaâ€™ are faster for large ones.
# For multiclass problems, only â€˜newton-cgâ€™, â€˜sagâ€™, â€˜sagaâ€™ and â€˜lbfgsâ€™ handle multinomial loss; â€˜liblinearâ€™ is limited to one-versus-rest schemes.
# â€˜newton-cgâ€™, â€˜lbfgsâ€™ and â€˜sagâ€™ only handle L2 penalty, whereas â€˜liblinearâ€™ and â€˜sagaâ€™ handle L1 penalty.
# We will use 'saga', a very efficient solver. You can read all about it here.
#
# Warm start
# Scikit-learn allows for multiple adjustments to the training. For example, you can try first with a little bit of data just to check if everything is working, and then, if you set warm_start = True before, it will retrain starting from the original parameters. Allows for dynamic updating as well. warm_start = False means whenever we give it new data, it will start from scratch, forgetting what it previously learned.

